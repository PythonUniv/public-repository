{
    "sources": [
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://pigsty.io/blog/pg/pg-eat-db-world/",
            "index": 0,
            "website_title": "Postgres is eating the database world | Pigsty",
            "website_description": " · The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios’ query patterns and performance demands. For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and …",
            "website_text": "Postgres is eating the database world\n\nPostgreSQL isn’t just a simple relational database; it’s a data management framework with the potential to engulf the entire database realm. The trend of “Using Postgres for Everything” is no longer limited to a few elite teams but is becoming a mainstream best practice.\n\n----------------------------------------\n\n\nOLAP’s New Challenger\n\nIn a 2016 database meetup, I argued that a significant gap in the PostgreSQL ecosystem was the lack of a sufficiently good columnar storage engine for OLAP workloads. While PostgreSQL itself offers lots of analysis features, its performance in full-scale analysis on larger datasets doesn’t quite measure up to dedicated real-time data warehouses.\n\nConsider ClickBench, an analytics performance benchmark, where we’ve documented the performance of PostgreSQL, its ecosystem extensions, and derivative databases. The untuned PostgreSQL performs poorly (x1050), but it can reach (x47) with optimization. Additionally, there are three analysis-related extensions: columnar store Hydra (x42), time-series TimescaleDB (x103), and distributed Citus (x262).\n\n\n> Clickbench c6a.4xlarge, 500gb gp2 results in relative time\n\nThis performance can’t be considered bad, especially compared to pure OLTP databases like MySQL and MariaDB (x3065, x19700); however, its third-tier performance is not “good enough,” lagging behind the first-tier OLAP components like Umbra, ClickHouse, Databend, SelectDB (x3~x4) by an order of magnitude. It’s a tough spot - not satisfying enough to use, but too good to discard.\n\nHowever, the arrival of ParadeDB and DuckDB changed the game!\n\nParadeDB’s native PG extension pg_analytics achieves second-tier performance (x10), narrowing the gap to the top tier to just 3–4x. Given the additional benefits, this level of performance discrepancy is often acceptable - ACID, freshness and real-time data without ETL, no additional learning curve, no maintenance of separate services, not to mention its ElasticSearch grade full-text search capabilities.\n\nDuckDB focuses on pure OLAP, pushing analysis performance to the extreme (x3.2) — excluding the academically focused, closed-source database Umbra, DuckDB is arguably the fastest for practical OLAP performance. It’s not a PG extension, but PostgreSQL can fully leverage DuckDB’s analysis performance boost as an embedded file database through projects like DuckDB FDW and pg_quack.\n\nThe emergence of ParadeDB and DuckDB propels PostgreSQL’s analysis capabilities to the top tier of OLAP, filling the last crucial gap in its analytic performance.\n\n----------------------------------------\n\n\nThe Pendulum of Database Realm\n\nThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios’ query patterns and performance demands.\n\nFor a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.\n\n\n> DDIA ch3, The republic of OLTP & Kingdom of OLAP\n\nLike many “specialized databases,” the strength of dedicated OLAP systems often lies in performance — achieving 1-3 orders of magnitude improvement over native PG or MySQL. The cost, however, is redundant data, excessive data movement, lack of agreement on data values among distributed components, extra labor expense for specialized skills, extra licensing costs, limited query language power, programmability and extensibility, limited tool integration, poor data integrity and availability compared with a complete DMBS.\n\nHowever, as the saying goes, “What goes around comes around”. With hardware improving over thirty years following Moore’s Law, performance has increased exponentially while costs have plummeted. In 2024, a single x86 machine can have hundreds of cores (512 vCPU EPYC 9754x2), several TBs of RAM, a single NVMe SSD can hold up to 64TB, and a single all-flash rack can reach 2PB; object storage like S3 offers virtually unlimited storage.\n\n\nHardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.\n\nAs DuckDB’s manifesto \"Big Data is Dead\" suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.\n\nIf 99% of use cases can now be handled on a single machine with standalone DuckDB or PostgreSQL (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive texts freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)\n\nThe shift in fundamental assumptions is steering the database world from a phase of diversification back to convergence, from a big bang to a mass extinction. In this process, a new era of unified, multi-modeled, super-converged databases will emerge, reuniting OLTP and OLAP. But who will lead this monumental task of reconsolidating the database field?\n\n----------------------------------------\n\n\nPostgreSQL: The Database World Eater\n\nThere are a plethora of niches in the database realm: time-series, geospatial, document, search, graph, vector databases, message queues, and object databases. PostgreSQL makes its presence felt across all these domains.\n\nA case in point is the PostGIS extension, which sets the de facto standard in geospatial databases; the TimescaleDB extension awkwardly positions “generic” time-series databases; and the vector extension, PGVector, turns the dedicated vector database niche into a punchline.\n\nThis isn’t the first time; we’re witnessing it again in the oldest and largest subdomain: OLAP analytics. But PostgreSQL’s ambition doesn’t stop at OLAP; it’s eyeing the entire database world!\n\n\nWhat makes PostgreSQL so capable? Sure, it’s advanced, but so is Oracle; it’s open-source, as is MySQL. PostgreSQL’s edge comes from being both advanced and open-source, allowing it to compete with Oracle/MySQL. But its true uniqueness lies in its extreme extensibility and thriving extension ecosystem.\n\n\n> Reasons users choose PostgreSQL: Open-Source, Reliable, Extensible\n\nPostgreSQL isn’t just a relational database; it’s a data management framework capable of engulfing the entire database galaxy. Besides being open-source and advanced, its core competitiveness stems from extensibility, i.e., its infra’s reusability and extension’s composability.\n\n----------------------------------------\n\n\nThe Magic of Extreme Extensibility\n\nPostgreSQL allows users to develop extensions, leveraging the database’s common infra to deliver features at minimal cost. For instance, the vector database extension pgvector, with just several thousand lines of code, is negligible in complexity compared to PostgreSQL’s millions of lines. Yet, this “insignificant” extension achieves complete vector data types and indexing capabilities, outperforming lots of specialized vector databases.\n\nWhy? Because pgvector’s creators didn’t need to worry about the database’s general additional complexities: ACID, recovery, backup & PITR, high availability, access control, monitoring, deployment, 3rd-party ecosystem tools, client drivers, etc., which require millions of lines of code to solve well. They only focused on the essential complexity of their problem.\n\nFor example, ElasticSearch was developed on the Lucene search library, while the Rust ecosystem has an improved next-gen full-text search library, Tantivy, as a Lucene alternative. ParadeDB only needs to wrap and connect it to PostgreSQL’s interface to offer search services comparable to ElasticSearch. More importantly, it can stand on the shoulders of PostgreSQL, leveraging the entire PG ecosystem’s united strength (e.g., mixed searches with PG Vector) to “unfairly” compete with another dedicated database.\n\n\n> Pigsty has 234 extensions available. And there are 1000+ more in the ecosystem\n\n----------------------------------------\n\nThe extensibility brings another huge advantage: the composability of extensions, allowing different extensions to work together, creating a synergistic effect where 1+1 » 2. For instance, TimescaleDB can be combined with PostGIS for spatio-temporal data support; the BM25 extension for full-text search can be combined with the PGVector extension, providing hybrid search capabilities.\n\nFurthermore, the distributive extension Citus can transparently transform a standalone cluster into a horizontally partitioned distributed database cluster. This capability can be orthogonally combined with other features, making PostGIS a distributed geospatial database, PGVector a distributed vector database, ParadeDB a distributed full-text search database, and so on.\n\n----------------------------------------\n\nWhat’s more powerful is that extensions evolve independently, without the cumbersome need for main branch merges and coordination. This allows for scaling — PG’s extensibility lets numerous teams explore database possibilities in parallel, with all extensions being optional, not affecting the core functionality’s reliability. Those features that are mature and robust have the chance to be stably integrated into the main branch.\n\nPostgreSQL achieves both foundational reliability and agile functionality through the magic of extreme extensibility, making it an outlier in the database world and changing the game rules of the database landscape.\n\n----------------------------------------\n\n\nGame Changer in the DB Arena\n\nThe emergence of PostgreSQL has shifted the paradigms in the database domain: Teams endeavoring to craft a “new database kernel” now face a formidable trial — how to stand out against the open-source, feature-rich Postgres. What’s their unique value proposition?\n\nUntil a revolutionary hardware breakthrough occurs, the advent of practical, new, general-purpose database kernels seems unlikely. No singular database can match the overall prowess of PG, bolstered by all its extensions — not even Oracle, given PG’s ace of being open-source and free.\n\nA niche database product might carve out a space for itself if it can outperform PostgreSQL by an order of magnitude in specific aspects (typically performance). However, it usually doesn’t take long before the PostgreSQL ecosystem spawns open-source extension alternatives. Opting to develop a PG extension rather than a whole new database gives teams a crushing speed advantage in playing catch-up!\n\nFollowing this logic, the PostgreSQL ecosystem is poised to snowball, accruing advantages and inevitably moving towards a monopoly, mirroring the Linux kernel’s status in server OS within a few years. Developer surveys and database trend reports confirm this trajectory.\n\n\n> StackOverflow 2023 Survey: PostgreSQL, the Decathlete\n\n\n> StackOverflow’s Database Trends Over the Past 7 Years\n\nPostgreSQL has long been the favorite database in HackerNews & StackOverflow. Many new open-source projects default to PostgreSQL as their primary, if not only, database choice. And many new-gen companies are going All in PostgreSQL.\n\nAs “Radical Simplicity: Just Use Postgres” says, Simplifying tech stacks, reducing components, accelerating development, lowering risks, and adding more features can be achieved by “Just Use Postgres.” Postgres can replace many backend technologies, including MySQL, Kafka, RabbitMQ, ElasticSearch, Mongo, and Redis, effortlessly serving millions of users. Just Use Postgres is no longer limited to a few elite teams but becoming a mainstream best practice.\n\n----------------------------------------\n\n\nWhat Else Can Be Done?\n\nThe endgame for the database domain seems predictable. But what can we do, and what should we do?\n\nPostgreSQL is already a near-perfect database kernel for the vast majority of scenarios, making the idea of a kernel “bottleneck” absurd. Forks of PostgreSQL and MySQL that tout kernel modifications as selling points are essentially going nowhere.\n\nThis is similar to the situation with the Linux OS kernel today; despite the plethora of Linux distros, everyone opts for the same kernel. Forking the Linux kernel is seen as creating unnecessary difficulties, and the industry frowns upon it.\n\nAccordingly, the main conflict is no longer the database kernel itself but two directions— database extensions and services! The former pertains to internal extensibility, while the latter relates to external composability. Much like the OS ecosystem, the competitive landscape will concentrate on database distributions. In the database domain, only those distributions centered around extensions and services stand a chance for ultimate success.\n\nKernel remains lukewarm, with MariaDB, the fork of MySQL’s parent, nearing delisting, while AWS, profiting from offering services and extensions on top of the free kernel, thrives. Investment has flowed into numerous PG ecosystem extensions and service distributions: Citus, TimescaleDB, Hydra, PostgresML, ParadeDB, FerretDB, StackGres, Aiven, Neon, Supabase, Tembo, PostgresAI, and our own PG distro — — Pigsty.\n\n----------------------------------------\n\nA dilemma within the PostgreSQL ecosystem is the independent evolution of many extensions and tools, lacking a unifier to synergize them. For instance, Hydra releases its own package and Docker image, and so does PostgresML, each distributing PostgreSQL images with their own extensions and only their own. These images and packages are far from comprehensive database services like AWS RDS.\n\nEven service providers and ecosystem integrators like AWS fall short in front of numerous extensions, unable to include many due to various reasons (AGPLv3 license, security challenges with multi-tenancy), thus failing to leverage the synergistic amplification potential of PostgreSQL ecosystem extensions.\n\n> Extesion Category   Pigsty RDS & PGDG     AWS RDS PG             Aliyun RDS PG\n> Add Extension       Free to Install       Not Allowed            Not Allowed\n> Geo Spatial         PostGIS 3.4.2         PostGIS 3.4.1          PostGIS 3.3.4\n> Time Series         TimescaleDB 2.14.2\n> Distributive        Citus 12.1\n> AI / ML             PostgresML 2.8.1\n> Columnar            Hydra 1.1.1\n> Vector              PGVector 0.6          PGVector 0.6           pase 0.0.1\n> Sparse Vector       PG Sparse 0.5.6\n> Full-Text Search    pg_bm25 0.5.6\n> Graph               Apache AGE 1.5.0\n> GraphQL             PG GraphQL 1.5.0\n> Message Queue       pgq 3.5.0\n> OLAP                pg_analytics 0.5.6\n> DuckDB              duckdb_fdw 1.1\n> CDC                 wal2json 2.5.3        wal2json 2.5\n> Bloat Control       pg_repack 1.5.0       pg_repack 1.5.0        pg_repack 1.4.8\n> Point Cloud         PG PointCloud 1.2.5   Ganos PointCloud 6.1\n> \n> Many important extensions are not available on Cloud RDS (PG 16, 2024-02-29)\n\nExtensions are the soul of PostgreSQL. A Postgres without the freedom to use extensions is like cooking without salt, a giant constrained.\n\nAddressing this issue is one of our primary goals.\n\n----------------------------------------\n\n\nOur Resolution: Pigsty\n\nDespite earlier exposure to MySQL Oracle, and MSSQL, when I first used PostgreSQL in 2015, I was convinced of its future dominance in the database realm. Nearly a decade later, I’ve transitioned from a user and administrator to a contributor and developer, witnessing PG’s march toward that goal.\n\nInteractions with diverse users revealed that the database field’s shortcoming isn’t the kernel anymore — PostgreSQL is already sufficient. The real issue is leveraging the kernel’s capabilities, which is the reason behind RDS’s booming success.\n\nHowever, I believe this capability should be as accessible as free software, like the PostgreSQL kernel itself — available to every user, not just renting from cyber feudal lords.\n\nThus, I created Pigsty, a battery-included, local-first PostgreSQL distribution as an open-source RDS Alternative, which aims to harness the collective power of PostgreSQL ecosystem extensions and democratize access to production-grade database services.\n\n\n> Pigsty stands for PostgreSQL in Great STYle, representing the zenith of PostgreSQL.\n\nWe’ve defined six core propositions addressing the central issues in PostgreSQL database services:\n\nExtensible Postgres, Reliable Infras, Observable Graphics, Available Services, Maintainable Toolbox, and Composable Modules.\n\nThe initials of these value propositions offer another acronym for Pigsty:\n\n> Postgres, Infras, Graphics, Service, Toolbox, Yours.\n> \n> Your graphical Postgres infrastructure service toolbox.\n\nExtensible PostgreSQL is the linchpin of this distribution. In the recently launched Pigsty v2.6, we integrated DuckDB FDW and ParadeDB extensions, massively boosting PostgreSQL’s analytical capabilities and ensuring every user can easily harness this power.\n\nOur aim is to integrate the strengths within the PostgreSQL ecosystem, creating a synergistic force akin to the Ubuntu of the database world. I believe the kernel debate is settled, and the real competitive frontier lies here.\n\n * PostGIS: Provides geospatial data types and indexes, the de facto standard for GIS (& pgPointCloud, pgRouting).\n * TimescaleDB: Adds time-series, continuous aggregates, distributed, columnar storage, and automatic compression capabilities.\n * PGVector: Support AI vectors/embeddings and ivfflat, hnsw vector indexes (& pg_sparse for sparse vectors).\n * Citus: Transforms classic master-slave PG clusters into horizontally partitioned distributed database clusters.\n * Hydra: Adds columnar storage and analytics, rivaling ClickHouse’s analytic capabilities.\n * ParadeDB: Elevates full-text search and mixed retrieval to ElasticSearch levels (& zhparser for Chinese tokenization).\n * Apache AGE: Graph database extension, adding Neo4J-like OpenCypher query support to PostgreSQL.\n * PG GraphQL: Adds native built-in GraphQL query language support to PostgreSQL.\n * DuckDB FDW: Enables direct access to DuckDB’s powerful embedded analytic database files through PostgreSQL (& DuckDB CLI).\n * Supabase: An open-source Firebase alternative based on PostgreSQL, providing a complete app development storage solution.\n * FerretDB: An open-source MongoDB alternative based on PostgreSQL, compatible with MongoDB APIs/drivers.\n * PostgresML: Facilitates classic machine learning algorithms, calling, deploying, and training AI models with SQL.\n\n\nDevelopers, your choices will shape the future of the database world. I hope my work helps you better utilize the world’s most advanced open-source database kernel: PostgreSQL.\n\n> Read in Pigsty’s Blog | GitHub Repo: Pigsty | Official Website",
            "source_score": 0.9368421052631579,
            "website_date": "2024-03-04",
            "searched_at": "2024-05-09T12:12:10.302070"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://medium.com/@satrianusa10/oltp-vs-olap-databases-understanding-the-difference-and-why-it-matters-be5e073937de",
            "index": 1,
            "website_title": "OLTP vs. OLAP Database Design Approach: Understanding the …",
            "website_description": " · Performance: OLTP databases are optimized for fast read and write operations, ensuring efficient transaction processing. OLAP databases, on the other hand, prioritize complex query performance and ...",
            "website_text": "OLTP vs. OLAP Database Design Approach: Understanding the Difference and Why It Matters | by Satria Aluh Perwira Nusa\n\nIn the world of data management and analytics, two common terms you’ll encounter are OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing) data processing. These two types of data processing serve distinct purposes and have different characteristics. Understanding the differences between OLTP and OLAP databases is crucial for designing efficient systems and making informed decisions based on data. In this article, we will explore the characteristics of OLTP and OLAP databases, highlighting why this distinction matters in the realm of data management and analysis.\n\nOLTP (Online Transaction Processing) Databases: OLTP databases are designed to handle real-time transactional operations, typically involving frequent read and write operations to a database. Here are the key features of OLTP databases:\n\n 1. Transactional Processing: OLTP databases focus on capturing and processing individual transactions, such as inserting, updating, and deleting records in a database. These transactions are usually short and involve a single or a few database records.\n 2. Operational Data: OLTP databases are optimized for managing operational data, which represents the day-to-day transactions and activities of an organization. Examples include customer orders, inventory management, and financial transactions.\n 3. High Concurrency: OLTP databases are built to handle a large number of concurrent transactions from multiple users simultaneously. They emphasize data integrity and ensure that concurrent transactions do not interfere with each other.\n 4. Normalized Data Structures: OLTP databases often employ normalized data structures, minimizing data redundancy and ensuring data consistency. This helps maintain the accuracy and integrity of the data.\n\nOLAP (Online Analytical Processing) Databases: OLAP databases, on the other hand, are designed for complex data analysis and decision-making. They provide a consolidated view of data from multiple sources and support advanced analytics. Here are the key features of OLAP databases:\n\n 1. Analytical Processing: OLAP databases focus on performing complex analytical queries and calculations on large volumes of data. They support functions such as aggregations, data mining, and advanced analytics to derive insights and make informed business decisions.\n 2. Historical and Aggregated Data: OLAP databases store historical and aggregated data, allowing users to analyze trends, patterns, and relationships over time. They provide a broader perspective on the data, enabling strategic decision-making.\n 3. Denormalized Data Structures: OLAP databases often employ denormalized data structures to optimize query performance. They store data in a way that facilitates efficient analysis and retrieval, even if it means duplicating some data.\n 4. Complex Queries: OLAP databases support complex queries involving multiple dimensions, hierarchies, and calculations. They allow for drill-down, slice-and-dice, and data pivoting operations to explore and analyze data from various perspectives.\n\nWhy It Matters: Understanding the distinction between OLTP and OLAP databases is crucial for several reasons:\n\n 1. Performance: OLTP databases are optimized for fast read and write operations, ensuring efficient transaction processing. OLAP databases, on the other hand, prioritize complex query performance and analytics over transactional speed.\n 2. Data Usage: OLTP databases focus on day-to-day operational data and support transactional workflows. OLAP databases handle analytical tasks, providing a consolidated view of data for strategic decision-making and business intelligence.\n 3. Database Design: The design and structure of OLTP and OLAP databases differ significantly. OLTP databases typically employ normalized data structures, while OLAP databases use denormalized structures to optimize query performance.\n 4. Business Applications: Choosing the appropriate database type is crucial for specific business applications. OLTP databases are suitable for transactional systems, such as e-commerce platforms or banking applications. OLAP databases are more applicable for data analysis, reporting, and business intelligence applications.\n\nThe distinction between OLTP and OLAP databases lies in their purpose, design, and functionality. OLTP databases excel at handling real-time transactional operations, while OLAP databases are designed for complex data analysis and decision-making. By understanding the differences between these database types, you can make informed decisions regarding database design, system architecture, and selecting the appropriate database for your specific business needs.",
            "source_score": 0.19473684210526315,
            "website_date": "2023-06-23",
            "searched_at": "2024-05-09T12:12:10.312072"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://medium.com/@michelle.xie/explain-by-example-oltp-vs-olap-d5603ac2038b",
            "index": 2,
            "website_title": "Explain By Example: OLTP vs. OLAP | by Michelle Xie | Medium",
            "website_description": " · OLTP: Online transaction processing information systems facilitates and manages transaction-oriented applications. OLAP: Online analytical processing is an approach to answer multi-dimensional ...",
            "website_text": "Explain By Example: OLTP vs. OLAP | by Michelle Xie\n\nI remember in my first semester of university, I came across ‘OLTP’ and ‘OLAP’ in my Information Systems course. I read the textbook definition and didn’t understand it. But rather than asking my professors about it, I memorized the definitions and prayed that it wouldn’t come up in the exams. A couple of years down the road, I came across it again and thought, “Maybe its time I tried to learn the differences” and what better way to do this than explain it to myself with an example. So here we go…\n\nLet’s go back to school:\n\nThis school contains students, teachers, and the principal. The school also interacts with parents. This school puts all their data into a single database. The students data, the teachers data, and the courses data all goes into the same database.\n\nIf you don’t know what a database is, it’s essentially a place that stores data in some sort of structured way. Think of a telephone book for example. If you have ever used a telephone book, you would know that a telephone book stores contact details (the data) in an alphabetical ordering (structured format). Except with this telephone book, you can make changes to it (cross out numbers and update it), you can add to it (add new contacts in), you can even remove some of the contact details (by ripping the pages out).\n\nAs you can imagine, over time, the school population grows, the school offerings grows so naturally the data it needs to store will also grow with it.\n\nIf a student has graduated from the school more than 5 years ago, do we really want to keep his or her data in the school systems?\n\nA. Yes\n\nB. No\n\nC. I don’t know\n\nThe answer is D for Data Warehouse.\n\nData warehouses are used to store aggregated data that is generally more high-level than the data found in a database and usually less real-time for analytical purposes. For example, it may store some of the data from the student that graduated more than 5 years ago so that the school can use it track things like:\n\nHow many students have graduated from this school in the last decade?\n\nThat’s a very simple use case but you get the idea. That student’s data may no longer be relevant in the database that should only really contain data that is up-to-date and relevant to the school’s current state but it doesn’t mean their data is completely irrelevant just because they left the school more than 5 years ago.\n\nSo, how does OLTP and OLAP come into play?\n\nBecause the type of data stored in a database is different to a data warehouse, it makes sense that the type of information a database can provide differs from the information that a data warehouse can provide.\n\nThink about your parents going to your school and asking your teachers and the principal the same questions:\n\n 1. How is my child doing in class?\n 2. Will my child succeed at this school?\n\nIf your parents asked your teacher question 1, they would probably get a response that is along the lines of,\n\n“Oh, Johnny is doing very well in class at the moment. You should be proud of him.”\n\nor,\n\n“Johnny is a bit of a slacker and keeps distracting his peers with this Pokemon Go nonsense”.\n\nIf your parents asked the principal question 1, they would probably respond with,\n\n“Who?”\n\nIf we think of the teacher as the database and the principal as the data warehouse, the teacher gets to see Johnny everyday. They teach and interact with Johnny everyday and so they would have the most up-to-date information about Johnny at school. The principal might be able to find out that information if they remembered which class Johnny was taught in but that’s really inefficient and you might run into the issue that the principal did not keep track of which class Johnny belongs to at all.\n\nOn the other hand, if your parents asked the teacher question 2, the teacher won’t have enough data to tell your parents straight away. They might be able to tell your parents eventually once they’ve hunted down all the teachers and found out their class averages or how well Johnny is doing in other classes.\n\nIf your parents asked the principal question 2, the principal will say straight away:\n\n“Our school has experienced a 95% passing rate over the last 5 years so I can extremely confident that your child will also succeed at our school.”\n\nThe principal (or data warehouse) aggregates all data necessary from students, teachers, and classes (the databases) over time to make big, bold claims (analytical statements) like that.\n\nSo far you probably have a better understanding of what OLTP vs. OLAP means than the definitions provided by Wikipedia:\n\nOLTP: Online transaction processing information systems facilitates and manages transaction-oriented applications.\n\nOLAP: Online analytical processing is an approach to answer multi-dimensional analytical queries swiftly.\n\nBasically, OLTP are the type of queries or operations executed on a database and OLAP are the type of queries executed in a data warehouse.\n\nLet’s say the school got its funding cut and the principal suddenly has to reduce the school staff. They know the mathematics department has been getting a lot of complaints lately so the principal simply asks the school database:\n\n“Which teacher should I get rid of based on the number of complaints?”\n\nNote: the principal is no longer acting as the data warehouse in this scenario but an actual human principal.\n\nOr in a happier scenario, the school receives a funding grant so now they have to choose which department to invest into. Let’s say the data warehouse is built to be able to answer that exactly:\n\nBy aggregating the data from their classes (educational departments) and finance departments in the data warehouse, the principal can execute OLAP queries to find that the science department has been heavily under-invested into and needs more investment.\n\nOne more thing worth mentioning is the concepts of Normalization vs. De-normalization:\n\nNormalization is essentially trying to make data as consistent as possible and reduce redundancy by trying to keep things that are related as separate as possible to reduce any repeats. The example that I like to use is that normalization is like having the parents call up every single teacher to find out all the grades of their child. De-normalization on the other hand is like having all the grades summarized up into a Report Card.\n\nHowever, what if one of the child’s grade changes after the Report Card is printed because the teacher had made a mistake in marking?\n\nSo in a way, Report Cards (or de-normalization) does not maintain data integrity. To produce the Report Card, it had to bring together the student’s data as well as the course and class data which means there is data redundancy (repeated information).\n\nOn the upside, the parents don’t have to do as much work (querying) to get an overall of their child’s progress at school.\n\nFunnily enough, Azure provides both a database and data warehouse offering which you should definitely check out (now that you understand the differences between the two):\n\n * Azure SQL Database\n * Azure SQL Data Warehouse (now called Azure Synapse Analytics)",
            "source_score": 0.14210526315789473,
            "website_date": "2020-01-29",
            "searched_at": "2024-05-09T12:12:10.330074"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://www.singlestore.com/blog/pre-modern-databases-oltp-olap-and-nosql/",
            "index": 3,
            "website_title": "Pre-Modern Databases: OLTP, OLAP, and NoSQL - SingleStore",
            "website_description": " · Query Patterns. Unlike the OLTP data access patterns that were relatively simple, the query patterns for analytics are a lot more complicated. Trying to answer a question such as, “Show me the sales of product X, grouped by region and sales team, over the last two quarters,” requires a query that uses more complex functions and joins …",
            "website_text": "Pre-Modern Databases: OLTP, OLAP, and NoSQL\n\nIn this blog post, the first in a two-part series, I’m going to describe pre-modern databases: traditional relational databases, which support SQL but don’t scale out, and NoSQL databases, which scale out but don’t support SQL. In the next part, I’m going to talk about modern databases – which scale out, and which do support SQL – and how they are well suited for an important new workload: operational analytics.\n\n\nIn the Beginning: OLTP\n\nOnline transaction processing (OLTP) emerged several decades ago as a way to enable database customers to create an interactive experience for users, powered by back-end systems. Prior to the existence of OLTP, a customer would perform an activity. Only at some point, relatively far off in the future, would the back-end system be updated to reflect the activity. If the activity caused a problem, that problem would not be known for several hours, or even a day or more, after the problematic activity or activities.\n\nThe classic example (and one of the main drivers for the emergence of the OLTP pattern) was the ATM. Prior to the arrival of ATMs, a customer would go to the bank counter to withdraw or deposit money. Back-end systems, either paper or computer, would be updated at the end of the day. This was slow, inefficient, error prone, and did not allow for a real-time understanding of the current state. For instance, a customer might withdraw more money than they had in their account.\n\nWith the arrival of ATMs, around 1970, a customer could self-serve the cash withdrawal, deposits, or other transactions. The customer moved from nine to five access to 24/7 access. ATMs also allowed a customer to understand in real time what the state of their account was. With these new features, the requirements for the backend systems became a lot more complex. Specifically data lookups, transactionality, availability, reliability, and scalability – the latter being more and more important as customers demanded access to their information and money from any point on the globe.\n\nThe data access pattern for OLTP is to retrieve a small set of data, usually by doing a lookup on an ID. For example, the account information for a given customer ID. The system also must be able to write back a small amount of information based on the given ID. So the system needs the ability to do fast lookups, fast point inserts, and fast updates or deletes.\n\nTransaction support is arguably the most important characteristic that OLTP offers, as reflected in the name itself. A database transaction means a set of actions that are either all completed, or none of them are completed; there is no middle ground. For example, an ATM has to guarantee that it either gave the customer the money and debited their account, or did not give the customer money and did not debit their account. Only giving the customer money, but not debiting their account, harms the bank; only debiting the account, but not giving the customer money, harms the customer.\n\nNote that doing neither of the actions – not giving the money, and not debiting the account – is an unhappy customer experience, but still leaves the system in a consistent state. This is why the notion of a database transaction is so powerful. It guarantees the atomicity of a set of actions, where atomicity means that related actions happen, or don’t happen, as a unit.\n\nReliability is another key requirement. ATMs need to be always available, so customers can use one at any time. Uptime for the ATM is critical, even overcoming hardware or software failures, without human intervention. The system needs to be reliable because the interface is with the end customer and banks win on how well they deliver a positive customer experience. If the ATM fails every few times a customer tries it, the customer will get annoyed and switch to another bank.\n\nScalability is also a key requirement. Banks have millions of customers, and they will have tens of thousands of people hitting the back-end system at any given time. But the usage is not uniform. There are peak times when a lot more people hit the system.\n\nFor example, Friday is a common payday for companies. That means many customers will all be using the system around the same time to check on the balance and withdraw money. They will be seriously inconvenienced – and very unimpressed – if one, or some, or all of the ATMs go down at that point.\n\nSo banks need to scale to hundreds of thousands of users hitting the system concurrently on Friday afternoons. Hard to predict, one-off events, such as a hurricane or an earthquake, are among other examples that can also cause peaks. The worst case is often the one you didn’t see coming, so you need a very high level of resiliency even without having planned for the specific event that ends up occurring.\n\nThese requirements for the OLTP workload show up in many other use cases, such as retail transactions, billing, enterprise resource planning (widely known as ERP), customer relationship management (CRM), and just about any application where an end user is reviewing and manipulating data they have access to and where they expect to see the results of those changes immediately.\n\nThe existing legacy database systems were founded to solve these use cases over the last few decades, and they do a very good job of it, for the most part. The market for OLTP-oriented database software is in the tens of billions of dollars a year. However, with the rise of the Internet, and more and more transactional systems being built for orders of magnitude more people, legacy database systems have fallen behind in scaling to the level needed by modern applications.\n\nThe lack of scale out also makes it difficult for OLTP databases to handle analytical queries while successfully, reliably, and quickly running transactions. In addition, they lack the key technologies to perform the analytical queries efficiently. This has contributed to the need for separate, analytics-oriented databases, as described in the next section.\n\nA key limitation is that OLTP databases have typically run on a single computing node. This means that the transactions that are the core of an OLTP database can only happen at the speed and volume dictated by the single system at the center of operations. In an IT world that is increasingly about scaling out – spreading operations across arbitrary numbers of servers – this has proven to be a very serious flaw indeed.\n\n\nOLAP Emerges to Complement OLTP\n\nAfter the evolution of OLTP, the other major pattern that has emerged is OLAP. OLAP emerged a bit after OLTP, as enterprises realized they needed fast and flexible access to the data stored in their OLTP systems.\n\nOLTP system owners could, of course, directly query the OLTP system itself. However, OLTP systems were busy with transactions – any analytics use beyond the occasional query threatened to bog the OLTP systems down, limited to a single node as they were. And the OLAP queries quickly became important enough to have their own performance demands.\n\nAnalytics use would tax the resources of the OLTP system. Since the availability and reliability of the OLTP system were so important, it wasn’t safe to have just anyone running queries that might use up resources to any extent which would jeopardize the availability and reliability of the OLTP system.\n\nIn addition, people found that the kinds of analytics they wanted to do worked better with a different schema for the data than was optimal for the OLTP system. So they started copying the data over into another system, often called a data warehouse or a data mart. As part of the copying process, they would change the database schema to be optimal for the analytics queries they needed to do.\n\nAt first, OLTP databases worked reasonably well for analytics needs (as long as they ran analytics on a different server than the main OLTP workload). The legacy OLTP vendors included features such as grouping and aggregation in the SQL language to enable more complex analytics. However, the requirements of the analytics systems were different enough that a new breed of technology emerged that could satisfy analytics needs better, with features such as column-storage and read-only scale-out. Thus, the modern data warehouse was born.\n\nThe requirements for a data warehouse were the ability to run complex queries very fast; the ability to scale to handle large data sets (orders of magnitude larger than the original data from the OLTP system); and the ability to ingest large amounts of data in batches, from OLTP systems and other sources.\n\n\nQuery Patterns\n\nUnlike the OLTP data access patterns that were relatively simple, the query patterns for analytics are a lot more complicated. Trying to answer a question such as, “Show me the sales of product X, grouped by region and sales team, over the last two quarters,” requires a query that uses more complex functions and joins between multiple data sets.\n\nThese kinds of operations tend to work on aggregates of data records, grouping them across a large amount of data. Even though the result might be a small amount of data, the query has to scan a large amount of data to get to it.\n\nPicking the right query plan to optimally fetch the data from disk requires a query optimizer. Query optimization has evolved into a specialty niche within the realm of computer science; there are only a small number of people in the world with deep expertise in it. This specialization is key to the performance of database queries, especially in the face of large data sets.\n\nBuilding a really good query optimizer and query execution system in a distributed database system is hard. It requires a number of sophisticated components including statistics, cardinality estimation, plan space search, the right storage structures, fast query execution operators, intelligent shuffle, both broadcast and point-to-point data transmission, and more. Each of these components can take months or years of skilled developer effort to create, and more months and years to fine-tune.\n\n\nScaling\n\nDatasets for data warehouses can get quite big. This is because you are not just storing a copy of the current transactional data, but taking a snapshot of the state periodically and storing each snapshot going back in time.\n\nBusinesses often have a requirement to go back months, or even years, to understand how the business was doing previously and to look for trends. So while operational data sets range from a few gigabytes (GBs) to a few terabytes (TBs), a data warehouse ranges from hundreds of GBs to hundreds of TBs. For the raw data in the biggest systems, data sets can reach petabytes (PBs).\n\nFor example, imagine a bank that is storing the transactions for every customer account. The operational system just has to store the current balance for the account. But the analytics system needs to record every transaction in that account, going back for years.\n\nAs the systems grew into the multiple TBs, and into the PB range, it was a struggle to get enough computing and storage power into a single box to handle the load required. As a result, a modern data warehouse needs to be able to scale out to store and manage the data.\n\nScaling out a data warehouse is easier than scaling an OLTP system. This is because scaling queries is easier than scaling changes – inserts, updates, and deletes. You don’t need as much sophistication in your distributed transaction manager to maintain consistency. But the query processing needs to be aware of the fact that data is distributed over many machines, and it needs to have access to specific information about how the data is stored. Because building a distributed query processor is not easy, there have been only a few companies who have succeeded at doing this well.\n\n\nGetting the Data In\n\nAnother big difference is how data is put into a data warehouse. In an OLTP system, data is entered by a user through interaction with the application. With a data warehouse, by contrast, data comes from other systems programmatically. Often, it arrives in batches and at off-peak times. The timing is chosen so that the work of sending data does not interfere with the availability of the OLTP system where the data is coming from.\n\nBecause the data is moved programmatically by data engineers, you don’t need the database platform to enforce constraints on the data to keep it consistent. Because it comes in batches, you want an API that can load large amounts of data quickly. (Many data warehouses have specialized APIs for this purpose.)\n\nLastly, the data warehouse is not typically available for queries during data loading. Historically, this process worked well for most businesses. For example, in a bank, customers would carry out transactions against the OLTP system, and the results could be batched and periodically pushed into the analytics system. Since statements were only sent out once a month, it didn’t matter if it took a couple of days before the data made it over to the analytics system.\n\nSo the result is a data warehouse that is queryable by a small number of data analysts. The analysts run a small number of complex queries during the day, and the system is offline for queries while loading data during the night. The availability and reliability requirements are lower than an OLTP system because it is not as big a deal if your analysts are offline. You don’t need transactions of the type supported by the OLTP system, because the data loading is controlled by your internal process.\n\n\nThe NoSQL Work Around\n\nFor more information on this topic, read our previous blog post: Thank You for Your Help, NoSQL, But We Got It from Here.\n\nAs the world “goes digital,” the amount of information available increases exponentially. In addition, the number of OLTP systems has increased dramatically, as has the number of users consuming them. The growth in data size and in the number of people who want to take advantage of the data has outstripped the capabilities of legacy databases to manage. As scale-out patterns have permeated more and more areas within the application tier, developers have started looking for scale-out alternatives for their data infrastructure.\n\nIn addition, the separation of OLTP and OLAP has meant that a lot of time, energy, and money go into extracting, transforming, and loading data – widely known as the ETL process – between the OLTP and OLAP sides of the house.\n\nETL is a huge problem. Companies spend billions of dollars on people and technology to keep the data moving. In addition to the cost, the consequence of ETL is that users are guaranteed to be working on stale data, with the newest data up to a day old.\n\nWith the crazy growth in the amount of data – and in demand for different ways of looking at the data – the OLAP systems fall further and further behind. One of my favorite quotes, from a data engineer at a large tech company facing this problem, is: “We deliver yesterday’s insights, tomorrow!”.\n\nNoSQL came along promising an end to all this. NoSQL offered:\n\n * Scalability. NoSQL systems offered a scale-out model that broke through the limits of the legacy database systems.\n * No schema. NoSQL abandoned schema for unstructured and semi-structured formats, abandoning the rigid data typing and input checking that make database management challenging.\n * Big data support. Massive processing power for large data sets.\n\nAll of this, though, came at several costs:\n\n * No schema, no SQL. The lack of schema meant that SQL support was not only lacking from the get-go, but hard to achieve. Moreover, NoSQL application code is so intertwined with the organization of the data that application evolution becomes difficult. In other words, NoSQL systems lack the data independence found in SQL systems.\n * No transactions. It’s very hard to run traditional transactions on unstructured or semi-structured data. So data was left unreconciled, but discoverable by applications, that would then have to sort things out.\n * Slow analytics. Many of the NoSQL systems made it very easy to scale and to get data into the system (i.e., the data lake). While these systems did allow the ability to process larger amounts of data than ever before, they are pretty slow. Queries could take hours or even tens of hours. It was still better than not being able to ask the question at all, but it meant you had to wait a long while for the answer.\n\nNoSQL was needed as a complement to OLTP and OLAP systems, to work around the lack of scaling. While it had great promise and solved some key problems, it did not live up to all its expectations.\n\n\nThe Emergence of Modern Databases\n\nWith the emergence of NewSQL systems such as SingleStore, much of the rationale for using NoSQL in production has dissipated. We have seen many of the NoSQL systems try to add back important, missing features – such as transaction support and SQL language support – but the underlying NoSQL databases are simply not architected to handle them well. NoSQL is most useful for niche use cases, such as a data lake for storing large amounts of data, or as a kind of data storage scratchpad for application data in a large web application.\n\nThe core problems still remain. How do you keep up with all the data flowing in and still make it available instantly to the people who need it? How can you reduce the cost of moving and transforming the data? How can you scale to meet the demands of all the users who want access to the data, while maintaining an interactive query response time?\n\nThese are the challenges giving rise to a new workload, operational analytics. Read our upcoming blog post to learn about the operational analytics workload, and how NewSQL systems like SingleStore allow you to handle the challenges of these modern workloads.",
            "source_score": 0.11089866156787763,
            "website_date": "2019-04-10",
            "searched_at": "2024-05-09T12:12:10.365072"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://seattledataguy.substack.com/p/oltp-vs-olap-transactions-vs-analytics",
            "index": 4,
            "website_title": "OLTP vs OLAP - Transactions Vs Analytics - Substack",
            "website_description": " · OLTP systems typically use normalized data models, where the data is organized into highly structured tables, and the relationships between the tables are defined through primary and foreign keys. This allows for efficient data access and manipulation, which is critical for transaction processing. In contrast, OLAP systems …",
            "website_text": "OLTP vs OLAP\n\nI have worked with several companies that initially started working with their OLTP to provide analytics. This will generally work early on, but these systems aren’t optimized to run complex queries.\n\nDatabases like MongoDB and CassandraDB also add in the complexity of not being SQL-friendly, which most analysts and data practitioners are accustomed to us…\n\n\nKeep reading with a 7-day free trial\n\nSubscribe to SeattleDataGuy’s Newsletter to keep reading this post and get 7 days of free access to the full post archives.",
            "source_score": 0.22073578595317725,
            "website_date": "2023-05-07",
            "searched_at": "2024-05-09T12:12:10.368071"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://stackoverflow.com/questions/25933904/how-are-olap-oltp-data-warehouses-analytics-analysis-and-data-mining-related",
            "index": 5,
            "website_title": "database - How are OLAP, OLTP, data warehouses, analytics, analysis and ...",
            "website_description": " · OLTP is used in transaction purpose, between database and software (usually only one way of input/output data). OLAP is for analitical purpose, and this means there is multiple sources, historical data, high select query performance, mined data. edit because of comment: Data Processing is way how data is stored and accessed from …",
            "website_text": "database - How are OLAP, OLTP, data warehouses, analytics, analysis and data mining related?\n\nI will try to explain you from the top of the pyramid:\n\nBusiness Intelligence (what you didn't mentioned) is term in IT which stands for a complex system and gives useful informations about company from data.\n\nSo, BI systems has target: Clean, accurate and meaningful informations. Clean means there is no tech problems (missing keys, incomplete data ect). Accurate means accurate - BI systems are also used as fault checker of production database (logical faults - i.e invoice bill is too high, or inactive partner is used ect). It has been accomplished with rules. Meaningful is hard to explain, but in simple english, it's all your data (even excel table from the last meeting), in way you want.\n\nSo, BI system has back-end: It's data warehouse. DWH is nothing else than a database (instance, not software). It can be stored in RDBMS, analytical db (columnar or document store types), or NoSQL databases.\n\nData warehouse is term used usually for whole database that I explained above. There could be number of data-marts (if Kimball model is used) - more often, or relational system in 3rd normalized form (Inmon model) called enterprise data warehouse.\n\nData marts are tables inside DWH that are related (star schema, snowflake schema). Fact table (business process in denormalized form ) and dimension tables.\n\nEach data mart represents one business process. Example: DWH has 3 data marts. One is retail sales, second is export, and third is import. In retail you can see total sales, qty sold, import price, profit (measures) by SKU, date, store, city ect (dimensions).\n\nLoading data in DWH is called ETL(extract, transform, load).\n\n 1. Extract data from multiple sources (ERP db, CRM db, excel files, web service...)\n\n 2. Transform data (clean data, connect data from diff sources, match keys, mine data)\n\n 3. Load data (Load transformed data in specific data marts)\n\nedit because of comment: ETL process is usually created with ETL tool, or manually with some programming language (python, c# ect) and APIs.\n\nETL process is group of SQLs, procedures, scripts and rules related and separated in 3 parts (look above), controlled by meta data. It's either scheduled (every night, every few hours) or live (change data capture, triggers, transactions).\n\nOLTP and OLAP are types of data processing. OLTP is used in transaction purpose, between database and software (usually only one way of input/output data). OLAP is for analitical purpose, and this means there is multiple sources, historical data, high select query performance, mined data.\n\nedit because of comment: Data Processing is way how data is stored and accessed from database. So, based on of your needs, database is set in different way.\n\nImage from http://datawarehouse4u.info/:\n\n\nData mining is the computational process of discovering patterns in large data sets. Mined data can give you more insight view of business process or even forecast.\n\nAnalysis is a verb, which in BI world means simplicity of getting asked information from data. Multidimensional analysis actually says how system is slicing your data (with dimensions inside cube). Wikipedia said that analysis of data is a process of inspecting data with the goal of discovering useful information.\n\nAnalytics is a noun and it represent a result of analysis process.\n\nDon't get so much fuss about those two words.",
            "source_score": 0.06578947368421052,
            "website_date": "2014-09-19",
            "searched_at": "2024-05-09T12:12:10.376035"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://dataproducts.substack.com/p/oltp-vs-olap-the-core-of-data-miscommunication",
            "index": 6,
            "website_title": "OLTP vs OLAP: The Core of Data Miscommunication - Substack",
            "website_description": " · Surprisingly, many engineers and data professionals are quite knowledgeable of the tooling landscape, such as MongoDB, Postgres, and Snowflake, but fewer are aware of the OLTP and OLAP terminology. In short, OLTP stands for “online transaction processing” and OLAP stands for “online analytical processing.”. Below is …",
            "website_text": "OLTP vs OLAP: The Core of Data Miscommunication\n\n👋 Hi folks, thanks for reading my newsletter! My name is Chad Sanderson, and I write about data, data products, data modeling, and the future of data engineering and data architecture. In today’s article, I’m inviting Mark Freeman to share his experience in managing the relationship between data producers and consumers. Please consider subscribing if you haven’t already, reach out on LinkedIn if you ever want to connect, and join our Slack community Data Quality Camp for practitioner-led advice on Data Contracts and Data Quality at scale!\n\nThe reason why data producers and consumers struggle to communicate is deeper than the data— it’s tied to how we structure our data infrastructure. In my last role as a data scientist, I was hired at an HR tech company specifically to translate between the engineering and research teams, essentially turning analytics and I/O psychology theory into data products. Despite both teams having the same ultimate goal of building amazing HR data products, I experienced firsthand the breakdown of communication and misunderstanding of needs amongst these two groups through this translation work. Upon talking to others in the data industry I quickly learned that this challenge wasn’t unique to my job, but rather an endemic issue across data teams of various company sizes.\n\nAs a data scientist within the product team, my world centered around cloud data warehouses such as BigQuery, and I constantly found myself struggling to explain to engineering teams how the way they worked with data was different from the data science team. Through my time putting code into production within the engineering codebase and driving product decisions via analytics in the data warehouse, I came to the following conclusion: The reason why data producers and consumers struggle to understand each others’ needs is due to their interactions with data being drastically different given that they respectively work within OLTP or OLAP data systems.\n\nIn this article I expand on this realization by describing the nuances of OLTP and OLAP data systems, how these systems shape the “data worldview” of data producers and consumers, and finally sharing how we can meet each team where they are to build scalable data products. In addition, I present the fictitious situation of Sell Online Corp grappling with the differences in understanding data through an OLTP or OLAP lens to make this distinction tangible. Finally, please note that the data producer and consumer relationship can go well beyond the data warehouse, but this article focuses on this stage in the data lifecycle given how common it is.\n\nSurprisingly, many engineers and data professionals are quite knowledgeable of the tooling landscape, such as MongoDB, Postgres, and Snowflake, but fewer are aware of the OLTP and OLAP terminology. In short, OLTP stands for “online transaction processing” and OLAP stands for “online analytical processing.” Below is an excellent table created by SeattleDataGuyshowing the difference between OLTP and OLAP databases (shared with his permission). I highly encourage checking out his article if you want to dive deeper into understanding this difference.\n\n\nOLTP data systems are typically the first and only database a company will have as it’s tied directly to the user actions associated with a product such as navigating an app, going through a workflow, or presenting information for a respective user. As a company grows and data increases, the business will start to ask questions about its own historical data. The OLTP system might be able to answer these questions but analytics workflows are expensive and require substantial transformations to make the data useful. To prevent the risk of taking down the critical role of OLTP systems for the live product, OLAP systems are utilized to replicate the data and utilize it for the expensive analytics processes. Thus, beginning the divide in data worldviews within the business.\n\nTo further drive this point, in this article, we will utilize the fake company Sell Online Corp to illustrate how data perspectives differ. In this use case, Sell Online Corp needs to present to their board how their major push for user reviews of products led to increased sales. This is a deceptively simple question that often uncovers the gaps in which different teams understand the semantics of their data.\n\nThere are three major components to consider for the OLTP side of data:\n\n 1. Data within an OLTP relational database should be in third-normalized form (3NF).\n\n 2. There is an emphasis on create, read, update, and delete (CRUD) tasks.\n\n 3. The database upholds atomicity, consistency, isolation, and durability (ACID)\n\nThese three major components ensure that there is low latency for information retrieval for serving the end user as quickly as possible while also being reliable.\n\nFor our use case of Sell Online Corp, we are going to utilize a monolith e-commerce architecture for its simplicity, as our focus is on data architecture rather than software architecture. Below is an example of what data the end user sees when viewing an item for purchase and the simple infrastructure behind it:\n\n\nIn the above example, our data infrastructure needs to quickly:\n\n * Present information about a specific product such as descriptions and price.\n\n * Determine the average stars for an item based on reviews.\n\n * Present the reviews in chronological order grouped by user.\n\n * Allow the user to write reviews and append updated reviews.\n\nTo achieve the above requirements, the data in third-normalized form would look the like the following tables within the OLTP database, with Average Stars being generated via on-the-fly aggregations:\n\n\nWhile OLTP best practices focus on 3NF for data modeling, OLAP reverses this structure to a “denormalized” form to answer questions that must scan across large swaths of data. In other words, OLAP data systems will have wide tables and numerous data transformations that don’t exist in the OLTP database— note that the data may not initially be stored as a wide table, but workflows among data consumers often lead to this format. Below is what the denormalized wide table would look like in the OLAP system utilized by Sell Online Corp’s data consumers along with ad-hoc tables:\n\n\nGoing back to the original business problem, “Sell Online Corp needs to present to their board how their major push for user reviews of products led to increased sales.” This deceptively simple question is extraordinarily difficult to answer as the following aspects are subjective:\n\n * What is considered a valid user?\n\n * What is considered a valid user review?\n\n * How do we calculate the average stars for a product?\n\n * At what scale of “increase” is relevant to business stakeholders?\n\n * Are user reviews driving changes in sales directly, or is there a specific relationship among various variables that lead to changes in sales (hint: it’s typically the latter)?\n\nIn the given scenario, the data is no longer utilized for straightforward CRUD operations that have explicit requirements tied to the product. Instead, the data is now used for complex analytical queries, where the requirements are refined as the comprehension of the underlying data improves. This differentiation results in a decoupling of perspectives regarding data usage between OLTP and OLAP users. This results in varied understandings of the data that eventually give rise to complexity and data quality issues.\n\nSo why do we even create this added complexity by introducing an OLAP database? Like most endeavors in data, it’s all about balancing tradeoffs where there is more value than risks to be gained from introducing complexity. In the use case of analytics workflows, data teams are able to “stress test” existing business logic to determine if the business logic is still valid, whether their underlying assumptions still hold true, or if there is additional logic that can be added or adjusted to create value. These updates to business logic is a business’s unique competitive advantage to better capture value in the market with data.\n\nIn our Sell Online Corp example above, there is a potential data quality issue related to “Average Stars” that’s not obvious at first glance until data teams in OLAP systems start unraveling the business logic. Specifically, there are three different ways to represent Average Stars that have implications for analytics. Below is a mock conversation with the engineering team at Sell Online Corp.\n\n\nNow this is a very specific use case, but imagine the level of complexity in logic that can be found amongst numerous product features and workflows within a mature product. Managing this complexity becomes quite the challenge as an organization scales and iterates in different directions.\n\n\nIn our Average Stars use case, why would the OLTP side opt for a simple calculation of all average stars rather than adding the additional logic to filter to the latest reviews by a user? This goes back to HOW data is operationalized among data producers (OLTP databases) and data consumers (OLAP databases), as well as the constraints of those teams.\n\nWithin an OLTP system, data producers care about:\n\n * Building specific product features with clear requirements.\n\n * ACID compliance among the data for CRUD operations.\n\n * Reducing unnecessary complexity so that the codebase is easy to build and maintain further features for the product.\n\nUnder these constraints, a valid reason for calculating Average Stars is for simplicity within the product codebase. The use case of someone updating the review for a product is a small fraction of overall reviews for Sell Online Corp; thus the added complexity of filtering for first or latest reviews by a user was not worth it. In other words, it was a product decision over a data decision, as the OLTP database serves the product with respect to user transactions.\n\nWithin an OLAP system, data consumers care about:\n\n * Finding unique relationships among data via the use of denormalized wide tables.\n\n * Teasing out the nuances of existing business logic to see how it can be validated and or improved.\n\n * Answering complex business questions with unclear requirements via an iterative process of understanding the underlying data.\n\nThough updated reviews account for a small fraction of overall reviews, the data teams on the OLAP side are uniquely positioned to see patterns among this usage. In the case of the OLTP side, data is organized in distinct chunks of information for transaction speed. But on the OLAP side, all the data is combined to surface interesting relationships that are not apparent in the OLTP team’s data. Specifically for Sell Online Corp, even though a small fraction of reviews are updated, the users who did update reviews were a very important population. Updated reviews were primarily on items greater than $300, as users were sensitive to perceived value for the price. In most cases, the bad reviews at this price point were later increased after the seller resolved the issue. This population accounted for a significant portion of revenue on Sales Online Corp.\n\nOften the data producers on the OLTP side and the data consumers on the OLAP side both have valid reasons for how they view the same data differently. What matters less is which side is right, and instead, we need to focus on what tradeoffs from both sides best serve the business via data.\n\nIf you want to review a real-world use case of these tradeoffs, then I highly encourage checking out Uber Freight Carrier Metrics with Near-Real-Time Analytics | Uber Blog where they describe the pros and cons of various OLTP and OLAP configurations on their Carrier App.\n\nThough Sell Online Corp is a fictitious example, this mimicked my experience in the previous HR tech company I worked in. One of the early mistakes I made was going into these conversations with only the analytics perspective in mind… this method didn’t get much traction.\n\nI became substantially more effective in my role when I left the comfort of my understanding through an OLAP lens and spent the time to understand the OLTP databases used by my data producer colleagues. I quickly learned that many of my “obvious” and “simple” asks were actually hard asks on my engineering colleagues as it increased the scope of their sprints and increased the complexity of their codebase. Such asks were a hard sell without making it clear to data producers their impact on the OLAP side used by data consumers.\n\nTherefore, the solution to this divide between OLTP and OLAP database users is not to force the benefits of one side or the other but to instead foster empathy so that both sides can translate the needs of each other. At this HR tech job, we made great strides by including the data consumers in the product requirements document (PRD) writing phase. It worked well because we added the OLAP perspective before data producers were constrained by existing requirements to deliver a product on a tight deadline. For the data consumers, we finally had an opportunity to see upstream and learn the product constraints that impacted our business logic.\n\nBut this approach can only take you so far. Without a way to scale these conversations and enforce the agreements made during the PRD writing phase, teams eventually revert back to the issue of data producers and consumers not understanding each others’ needs after a few iterations on a product feature. This is what ultimately led me to data contracts as the avenue in which to scale and maintain the empathy between OLTP and OLAP database users. Chad has written extensively about data contracts in this newsletter— read this article to dive deeper into data contracts— but in summary, data contracts are an API that enforces agreements, between data producers and consumers, regarding expectations of data assets to prevent known data quality issues. While this article doesn't deeply cover data contracts, I encourage folks to focus less on the technical implementation of data contracts and more on their potential to foster empathy among teams and enhance cooperation between OLTP data producers and OLAP data consumers.",
            "source_score": 0.13157894736842105,
            "website_date": "2023-06-23",
            "searched_at": "2024-05-09T12:12:10.406033"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://medium.com/analytics-vidhya/olap-and-oltp-the-two-different-data-techniques-78c91e0ab9d6",
            "index": 7,
            "website_title": "OLAP and OLTP — The two different data techniques - Medium",
            "website_description": " · OLAP systems can be implemented using relational databases, and this technique is often named ROLAP (Relational OLAP). But for that, we need to design the database not in the 5th normal form but ...",
            "website_text": "OLAP and OLTP — The two different data management techniques | by Kumar Brar | Analytics Vidhya\n\nImagine the very early days of computer systems, there was no need of complex systems like RDBMS or Data Warehouses. In those days, data was stored in a single file and everything was quite straightforward to implement. However, with the evolution of Information Age, computers and software started to take our life by storm and they become the need of every business. With it started the era of Relational Database Management System (RDBMS) for the storage and management of an organization’s data. Till today, this is one of the effective ways of storing and extracting data within an organization.\n\nAlthough, with the advent of Internet, we are no longer restricted to normal RDBMS systems. This is because the new age tools and apps store and access data in domains with no clear relationships and sometimes this is not even required (eg : to handle volatile data growth). In these scenarios we make use of NoSQL/MongoDB etc.\n\nNevertheless, a few terms of the old world are still used today, and it’s important to look at them with a modern approach. Two of them are, precisely, OLTP and OLAP. But for an overall context, let’s take a look at the following image that shows the relation between OLTP and OLAP.\n\nFrom the image, you can clearly say that OLTP and OLAP are not competing approaches to the same issue, but processes that complement each other. Next you’ll find a more in depth explanation of each one of those terms.\n\n\nOLTP\n\nThe term OLTP refers to Online Transaction Processing. It’s often used to mention databases that store and manage data relevant to the day-to-day operations of a system or company. An example of OLTP system is “ATM”.\n\nAs the information being stored on an OLTP data store was often critical to the business, a huge effort was put to ensure the Atomicity, Consistency, Isolation and Durability (ACID) of the data. Data stored according to these four principles are marked as ACID compliant, and this is where relational database management systems excel.\n\n> For eg : In our ATM machine, we need to ensure the principle of ACID which is an example of OLTP system. See the below example for clarity.\n> \n> Let’s consider an example of two members of a family having two ATM cards linked to the same account and the account balance is $500 say. Now, if both of them go to withdraw the entire amount, then as per ACID principle, the first person should be able to withdraw and the second person should get the message of “Insufficient Funds”. In a second scenario, if the transaction of first person gets failed (say due to server failure), then the second person should be able to withdraw the money. In this way, the account information should always be up-to-date to avoid any malfunctioning.\n\nBut with the advent of web, we are no longer restricted to just the old definition of OLTP and now-a-days, we store the data on non-relational databases as well. Most of those data stores comply only with some of the four principles of ACID. But depending on the use case, it’s OK to relax on one or more of these principles in exchange for other benefits (speed, scalability, etc.).\n\nSo, say, if we have an app monitoring the number of registered users or likes on a particular product, we can make use of NoSQL or MongoDB with some of the principles of ACID and still consider it as an OLTP system.\n\n\nOLAP\n\nThe term OLAP refers to Online Analytical Processing, and is often used to mention databases that store and manage data relevant for data analysis and decision making.\n\nOLAP is strongly connected to Business Intelligence (BI), a specialization of software development targeted at delivering applications for business analysis. An example of OLAP system is financial reporting for different departments during different time-intervals.\n\nThe biggest advance that this area has brought is the capacity to generate reports on the fly. It ended the need to call the IT department to ask for a custom report, or to automate the generation of specific reports. A BI system can now answer questions that the developers didn’t had the need to know it in advance that the question was going to be asked.\n\nBI systems are made possible by organizing the data in a form called Hypercube. This form explores the many dimensions of the data, and allows users to aggregate or drill down data by navigating the dimensions of the cube.\n\nOLAP systems can be implemented using relational databases, and this technique is often named ROLAP (Relational OLAP). But for that, we need to design the database not in the 5th normal form but in the 3rd normal form.\n\nWe can live with redundant data when analyzing data. What really matters is the capacity to navigate through the dimensions of the data. And this is where ROLAP shines, as a database schema in the 3rd normal form is suited for aggregations and drill downs.\n\n\nConclusion\n\nWhen encountering the terms OLTP and OLAP for the first time, it’s easy to question: which one is better? When in fact one should be asking: how does one complement the other?\n\nWe now know that:\n\n * OLTP is used to store and manage data for day-to-day operations;\n * OLAP is used to analyze that data.",
            "source_score": 0.10740203193033382,
            "website_date": "2020-06-24",
            "searched_at": "2024-05-09T12:12:10.417034"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://www.dataonfocus.com/oltp-vs-olap/",
            "index": 8,
            "website_title": "OLTP vs OLAP: Definitions and Comparison - DataOnFocus",
            "website_description": "WEBThe first obvious difference is the focus of each. OLTP is best suited to update existing data, becoming the logical choice to operational systems which work with every day actions. In opposite, OLAP provides great analysis habilities of stored information, making this kind of systems perfect for reporting purposes.",
            "website_text": "OLTP vs OLAP: Definitions and Comparison\n\nInformation is  the number one asset in digital transformation.  One of the most important subjects regarding information systems is the difference between OLAP and OLTP.\n\nSo, first we prepared a lot of information about OLAP and OLTP, concluding the resource with a comparative analysis between them.\n\nLet’s jump right away to the learning process!\n\nIn the next chapters, we’ll be describing each topic in a complete, yet simple way. Before going any further on these topics, we show you a simple infographic comparing the two approaches:\n\n\nWhat is OLAP?\n\nOnline analytical processing is a computer technology term referring to systems focused on analysing data in a specific database. This kind of systems are characterized for their analytical capabilities, addressing multi-dimensional or one dimension data, processing all the information. The standard applications of OLAP are bussiness intelligence, data writing and reporting, throught data mining processes.\n\nOLAP operations and databases\n\nOn the database level, these systems operation is defined by a low level of transactions, dealing with archived and historical information. This data is seldom updated, identifying the SELECT database operation as the key feature of the system. Therefore, this kind of databases are based on READ operations, aggregating all available information.\n\nDatabases that work as data warehouses apply this methodology, optimizing reading and aggregation operations of its multidimensional data model. Thus providing a great support for data analysis and reporting operations, critical in these kind of databases.\n\nData cube\n\nThe main component of these systems is a OLAP cube. A cube consists in combining data warehouse’s structures like facts and dimensions. Those are organized as schemas: star schema, snowflake schema and fact constellation. The merging of all the cubes creates a multidimensional data warehouse.\n\nSystem types\n\nThere are many types of OLAP systems, depending on it’s structure characteristics. The most common ones are: MOLAP, ROLAP and HOLAP.\n\nThe most important real world applications of these systems are: bussiness management and reporting, financial reporting, marketing, research and another data related issues. These processes are growing faster on these days, making them absolutely critical in a world that is becoming dependent of data. In the next paragraph we have a real world example of what we described before.\n\nReal World Example: In a hospital there is 20 years of very complete patient information stored. Someone on the administration wants a detailed report of the most common deseases and sucess rate of treatment. To achieve that, the application of OLAP operations to the hospital’s data warehouse is the key. Usually complex queries on the historical data compile that report, providing the means to the board analyse the information.\n\nWhat is OLTP?\n\nOnline Transaction Processing is a information system type that prioritizes transaction processing, dealing with operational data. This kind of computer systems are identified by the large number of transactions they support. Thus making them the best to address online application. The main applications of this method are all kind of transactional systems like databases, commercial, hospital applications and so on.\n\nIn a simple way, these systems gather input information and store them on a database, in a large scale. Most of today’s applications are based on this interaction methodology, with implementations of centralized or descentralized systems.\n\nOLTP database and operations\n\nOn the database level, these transactional systems base their operation on multi-access, fast and effective querys to the database. The most used operations are INSERT, UPDATE and DELETE, since they are directly modifying the data, providing new information on new trasactions. So, in these systems, data is frequently updated, requiring a effective write operations support.\n\nOne special characteristic of those databases is the standarizarion of it’s data. This happens because data normalization provides a faster and more effective way to perform database writes. The main concern is the atomicity of the trasanctions and ensuring that concurrent accesses don’t damage data. Addicionally, it is also important that it don’t degradate system’s performance.\n\nOther systems\n\nOLTP is not only about databases, but also other types of interaction mecanisms. All client-server architectures are examples of these processes, benefiting of the fast transaction and concurrent models. Descentralized systems are also online transaction processing, as all broker programs and web servervices are transaction oriented.\n\nReal World Example: A banking transaction system is a classic example. There are many users executing operations into their accounts and the system must guarantee the completeness of those actions. In this case, there are several concurrent transactions at the same time. Therefore, data coherence and efficient operations are the main goal.\n\nComparing OLTP vs OLAP\n\nOLTP, also known as Online Transaction Processing, and OLAP which stands for Online Analytical Processing, are two distinct kinds of information systems technologies.\n\nBoth are related to information databases, which provide the means and support for these two types of functioning.\n\nEach one of the methods creates a different branch on data management system, with it’s own ideas and processes, but they complement themselves.\n\nTo analyse and compare them we’ve built this resource!\n\nBasically, OLAP and OLTP are very different approaches to the use of databases, but not only. In one hand, online analytical processing is more focused on data analysis and reporting. On the other hand online trasaction processing target a transaction optimized system, with a lot of data changes.\n\nFor someone learning about data sciences, related to IT methods, it is important to know the difference between these two approaches to information. This is the base idea to systems like business intelligence, data mining, data warehousing, data modelling, etl processes and big data.\n\nRegarding the previous descriptions of the systems, we can compare them in a lot of distinct categories.\n\nThe review is detailed in the next table. Then we have a further discussion on each compared item which could evoke some doubts, to ensure you understood.\n\nCategory       Online Transactional Processing                              Online Analytical Processing\nData Source    Operational information of the application. Generally this   Historical and archive data.\n               processes are the source of the data.\nFocus          Updating data.                                               Reporting and retrieval of information.\nApplications   Management, operational, web services, client-server.        Management Systems. Reporting, decision.\nUsers          Common person. Staff.                                        Managers, executives, data scientists, marketers.\nData task      Operational tasks. Business tasks.                           Reporting and data analysis.\nData Refresh   Insert, update and delete operations. They are performed     Refreshing of data with huge data sets. Takes time and is\n               fast. Immediate results.                                     sporadic.\nData Model     Entity-relantionship on databases.                           One or multi-dimensional data.\nSchema         Normalized schemas. Many tables and relationships.           Star, snowflake and constellation schema. Fewer tables not\n                                                                            normalized.\nBackup         Regular backups with full, incremental and archives. This    Simple backup or realoading of the mecanisms that support\n               data is critical and can't be loss.                          data insertion.\nHorizon        Day-to-day, weeks, months.                                   Long time data.\nQueries        Simple, returning the results expected for the system        Complex queries of data in order to aggregate information\n               activity.                                                    for reporting.\nSpeed          Fast. Requires some indexes on large tables.                 Slow. Depending on the amount of data. Requires more\n                                                                            indexes.\nSpace          Operational data stored, tipically small.                    Large data sets of historical information. Large storage\n                                                                            needed.\n\nThe first obvious difference is the focus of each. OLTP is best suited to update existing data, becoming the logical choice to operational systems which work with every day actions. In opposite, OLAP provides great analysis habilities of stored information, making this kind of systems perfect for reporting purposes. They usually base their work in historical data, which is best to analyse, getting more accurate reports.\n\nThese systems have very different purposes.\n\nTo achiveve the better performance on each one, there are some architectures that suit best operational actions, and others that boost reporting and analysis. Then OLTP database schemas are usually on a normalized form, providing a better performance for the usual queries. On the other hand, OLAP databases have specific data warehouse organization schemas.\n\nBased on this first and critical differences, we can infer some other contrasts.\n\nIn OLTP, where the common work is operational, the queries on the system should be much more simpler than on OLAP, where complex digging on data is performed. Therefore, OLTP systems tend to be much faster than OLAP ones.\n\nOne of the subjects that hasn’t been discussed yet is data refresh.\n\nIn the OLTP system information is constantly changing, so this refresh rate is immediate after each actions. In the other hand, on OLAP systems, the refresh is a pre-defined processing job that stores large sets of data simultaneously. This take long time because of the data size and the process of normalizing the information.\n\nAnother term we didn’t discuss is the backup situation.\n\nSince data on OLTP systems is absolutely critical, it needs a complex backup system. Full backups of the data combined with incremental backups are required. OLAP only needs a backup from time to time, since it’s data is not critical and doesn’t keep the system running.\n\nSpace requirements on both sides is, obviously dependent on the size of information stored. However is safe to assume that historical data will need more storage space, since a lot more information is stored.",
            "source_score": 0.11089866156787763,
            "website_date": null,
            "searched_at": "2024-05-09T12:12:10.451067"
        },
        {
            "text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "searched_by_text": "The Pendulum of Database RealmThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios' query patterns and performance demands.For a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.",
            "query_url": "https://www.bing.com/search?q=The%20Pendulum%20of%20Database%20RealmThe%20distinction%20between%20OLTP%20and%20OLAP%20didn’t%20exist%20at%20the%20inception%20of%20databases.%20The%20separation%20of%20OLAP%20data%20warehouses%20from%20databases%20emerged%20in%20the%201990s%20due%20to%20traditional%20OLTP%20databases%20struggling%20to%20support%20analytics%20scenarios'%20query%20patterns%20and%20performance%20demands.For%20a%20long%20time,%20best%20practice%20in%20data%20processing%20involved%20using%20MySQL/PostgreSQL%20for%20OLTP%20workloads%20and%20syncing%20data%20to%20specialized%20OLAP%20systems%20like%20Greenplum,%20ClickHouse,%20Doris,%20Snowflake,%20etc.,%20through%20ETL%20processes.&setmkt=en-US&count=10",
            "website_link": "https://metriql.com/blog/2021/09/07/olap-in-modern-data-stack/",
            "index": 9,
            "website_title": "OLAP in the modern data stack | Metriql Docs",
            "website_description": " · Relational OLAP (ROLAP) With the new modern data stack, companies usually start collecting all their data into a modern data warehouse. Essentially, the data warehouse becomes their single source of truth. Duplicating the data from the single source of truth usually leads to data inconsistency issues, and copying massive datasets is …",
            "website_text": "OLAP in the modern data stack\n\nOLAP is almost 30 years old and has often been used in different contexts over the last 10 years. It's a huge market, and there are many enterprise software in the space. OLAP is simply pre-aggregated data from your raw data, and it's not for everyone. If you're working for a startup that doesn't have terabytes of data, you probably don't need OLAP because you can efficiently run ad-hoc queries on your raw data (also known as fact tables). However, when you have petabytes of data and have tens of people who rely on it to make decisions, you don't want to make them wait 10 minutes for a simple query. Let's breakdown the OLAP into two categories:\n\n\nMultidimensional OLAP (MOLAP)#\n\nMost of them have their proprietary storage and compute engines and provide fast access to data across different dimensions. You store your data in your data lake or warehouse, duplicate it into their system, and slice/dice the data using their query engine. We call this type of OLAP Multidimensional OLAP (MOLAP) because they have their storage/compute engine to run the queries. Apache Kylin, Druid, and Pinot are the open-source products in this space and well-known enterprise alternatives are Arcadia Data, Atscale, Kyligence.\n\nThe good part is that they can guarantee to run ad-hoc queries in less than a few hundred milliseconds, so they're pretty helpful in speeding up your BI tools or building a customer-facing application. On the other hand, it's not easy to set up and maintain the open-source alternatives; enterprise solutions are expensive, considering you can access the data from your data warehouse as well. You need to duplicate your data from your database, which causes a data silo. They sit in between the data tools (BI tools in general) and your data warehouse and cache the data to speed up the queries.\n\n\nRelational OLAP (ROLAP)#\n\nWith the new modern data stack, companies usually start collecting all their data into a modern data warehouse. Essentially, the data warehouse becomes their single source of truth. Duplicating the data from the single source of truth usually leads to data inconsistency issues, and copying massive datasets is slow and expensive in most cloud providers. Luckily, modern data warehouses are efficient enough to answer ad-hoc queries; you don't need to run Hadoop jobs that take hours/days to finish anymore. However, they also can't guarantee performance for ad-hoc queries. You need to leverage their advanced features to be able to cover OLAP use-cases affordably. Here are some of these techniques:\n\n1. BigQuery: BI Engine#\n\nBigQuery is a fully-managed data warehouse solution from Google. You pay for the data processing for each query that you're running. If you're running ad-hoc queries on raw data, it becomes expensive, and you can't guarantee to run the queries in less than a few seconds. Luckily, you can create dimensional tables from your raw data under a schema and buy BI Engine slots that store all the data in your dimensional tables in memory and let you run OLAP queries efficiently for a fixed price.\n\n2. ClickHouse: Aggregating Merge Tree#\n\nClickHouse is an open-source OLAP database from Yandex. It supports SQL, and the performance is mind-blowing given that it doesn't use GPUs. It also offers some exciting table layouts for pre-aggregating the data storing the intermediate state inside database tables. AggregatingmMergeTree lets you create views and update them transparently when you insert data into the raw tables. It's suitable for immutable time-series data such as customer event data, and it makes sense given that Yandex initially developed ClickHouse for their Google Analytics alternative, Yandex Metrica.\n\n3. Snowflake: Search Optimization Service#\n\nIf you heard about the term modern data stack before, you probably know about Snowflake. While they're not the first to come up with the concept of separating the compute and storage layer in the data warehouse industry (looking at you, Trino!), they proved that the approach scales well. It's usually cheaper than the alternatives if you have petabytes of data and want to run ad-hoc analytics queries on your raw data. However, you need too many compute resources to run ad-hoc queries concurrently, so it's still better to transform the data and create dimensional tables from the raw data. If you enable the search optimization service that enables fast access to random rows, the latency matches the MOLAP engines.\n\n4. Postgresql: Grouping sets & B-tree Indexing#\n\nOur old friend, Postgresql is an excellent solution for serving the pre-aggregated data. For raw data, you can leverage advanced techniques such as partitioning and BRIN indexes. Still, since Postgresql stores the data in a row-oriented format and doesn't have vectorized execution, the latency will not be comparable to cloud data warehouses. If you pre-aggregate your data with grouping sets and index the dimension columns, you can access the pre-aggregated metrics efficiently.\n\n5. Materialized Tables#\n\nSnowflake has materialized views, BigQuery has materialized views, and Redshift recently added support for materialized views. I didn't try it yet, but Snowflake's materialized views don't work reliably for my use case. They're usually not transparent, and they can't guarantee the low-latency ad-hoc queries to materialized views because they need to update the materialized tables every time you insert data. It is usually better to use a transformation tool such as dbt or Airflow to transform the data and update the tables manually and transparently and use lambda views for more deterministic and reliable workloads. Most data analysts that I talked to don't like black-boxes nowadays.\n\n6. Materialize.io: Streaming Materialized Views#\n\nMaterialize is not a database; it's a streaming engine. In the OLAP world, near-real-time workloads are usually enough to cover most of the use-cases but if you have real-time use-cases (< 1 min), Materialize's materialized views are always up-to-date and suitable for low-latency workloads. The downside is that it doesn't work inside your data warehouse; you need to set up and maintain it separately.\n\n7. [New] Firebolt: Aggregating and Join Indexes#\n\nFirebolt is a next-generation data warehouse that claims to be orders of magnitude faster than Snowflake. Their real power is to be able to create aggregating, join, and partial indexing on data. While the performance will probably be comparable to Snowflake for ad-hoc queries, if you create relevant indexes in your tables, it should be much faster as the partial computation is done before you run the query. It looks great on paper, but I didn't try the product yet. (I'd love to, though!) If these new indexing types don't really affect the insert/update/delete performance, I believe that it's the next big thing in the data warehouse industry.\n\n\nWhere does the metrics layer fit in OLAP?#\n\nThe metrics layer is just a proxy layer for your single source of truth, it should not duplicate the data, so we fit in the ROLAP engines. Except for 5 and 6, you need to transform and pre-aggregate the data separately. The transformation tools in the modern data stack such as dbt and Airflow help a lot. Still, you need to maintain all the roll-up models manually and deal with non-additive aggregates separately. Claire from Analytics Engineers Club recently wrote a great article about the problem. There is also Tristan's LookML vs dbt article that you should read about this topic.\n\nOnce you define your metrics in metriql, you can create Aggregates transparently; it creates the dbt models automatically inside your dbt project and uses them for the ad-hoc queries wherever they're applicable. Therefore we believe that if you use Metriql in combination with the technologies explained under the ROLAP section, the end-solution gets the best of the two worlds of MOLAP and ROLAP:\n\n * Low-latency but affordable.\n * Tight integration with the existing BI tools.\n * Doesn't suffer from the data silo issue.\n * Lightweight proxy to your single source of truth.\n\nWe will write tutorials about all these products and share the benchmarks, stay tuned. 🤞",
            "source_score": 0.12619502868068833,
            "website_date": "2021-09-07",
            "searched_at": "2024-05-09T12:12:10.468068"
        }
    ]
}