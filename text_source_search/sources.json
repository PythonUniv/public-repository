{
    "text": "Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.As DuckDB’s manifesto “Big Data is Dead” suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.If 99% of use cases can now be handled on a single machine with standalone PostgreSQL / DuckDB (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive text freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)",
    "searched_by_text": "Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.As DuckDB’s manifesto “Big Data is Dead” suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.If 99% of use cases can now be handled on a single machine with standalone PostgreSQL / DuckDB (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive text freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)",
    "query_url": "https://www.bing.com/search?q=Hardware%20advancements%20have%20solved%20the%20data%20volume%20and%20performance%20issue,%20while%20database%20software%20developments%20(PostgreSQL,%20ParadeDB,%20DuckDB)%20have%20addressed%20access%20method%20challenges.%20This%20puts%20the%20fundamental%20assumptions%20of%20the%20analytics%20sector%20—%20the%20so-called%20“big%20data”%20industry%20—%20under%20scrutiny.As%20DuckDB’s%20manifesto%20“Big%20Data%20is%20Dead”%20suggests,%20the%20era%20of%20big%20data%20is%20over.%20Most%20people%20don’t%20have%20that%20much%20data,%20and%20most%20data%20is%20seldom%20queried.%20The%20frontier%20of%20big%20data%20recedes%20as%20hardware%20and%20software%20evolve,%20rendering%20“big%20data”%20unnecessary%20for%2099%%20of%20scenarios.If%2099%%20of%20use%20cases%20can%20now%20be%20handled%20on%20a%20single%20machine%20with%20standalone%20PostgreSQL%20/%20DuckDB%20(and%20its%20replicas),%20what’s%20the%20point%20of%20using%20dedicated%20analytics%20components?%20If%20every%20smartphone%20can%20send%20and%20receive%20text%20freely,%20what’s%20the%20point%20of%20pagers?%20(With%20the%20caveat%20that%20North%20American%20hospitals%20still%20use%20pagers,%20indicating%20that%20maybe%20less%20than%201%%20of%20scenarios%20might%20genuinely%20need%20“big%20data.”)&setmkt=en-US&count=10",
    "sources": [
        {
            "text": "Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.As DuckDB’s manifesto “Big Data is Dead” suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.If 99% of use cases can now be handled on a single machine with standalone PostgreSQL / DuckDB (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive text freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)",
            "searched_by_text": "Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.As DuckDB’s manifesto “Big Data is Dead” suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.If 99% of use cases can now be handled on a single machine with standalone PostgreSQL / DuckDB (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive text freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)",
            "query_url": "https://www.bing.com/search?q=Hardware%20advancements%20have%20solved%20the%20data%20volume%20and%20performance%20issue,%20while%20database%20software%20developments%20(PostgreSQL,%20ParadeDB,%20DuckDB)%20have%20addressed%20access%20method%20challenges.%20This%20puts%20the%20fundamental%20assumptions%20of%20the%20analytics%20sector%20—%20the%20so-called%20“big%20data”%20industry%20—%20under%20scrutiny.As%20DuckDB’s%20manifesto%20“Big%20Data%20is%20Dead”%20suggests,%20the%20era%20of%20big%20data%20is%20over.%20Most%20people%20don’t%20have%20that%20much%20data,%20and%20most%20data%20is%20seldom%20queried.%20The%20frontier%20of%20big%20data%20recedes%20as%20hardware%20and%20software%20evolve,%20rendering%20“big%20data”%20unnecessary%20for%2099%%20of%20scenarios.If%2099%%20of%20use%20cases%20can%20now%20be%20handled%20on%20a%20single%20machine%20with%20standalone%20PostgreSQL%20/%20DuckDB%20(and%20its%20replicas),%20what’s%20the%20point%20of%20using%20dedicated%20analytics%20components?%20If%20every%20smartphone%20can%20send%20and%20receive%20text%20freely,%20what’s%20the%20point%20of%20pagers?%20(With%20the%20caveat%20that%20North%20American%20hospitals%20still%20use%20pagers,%20indicating%20that%20maybe%20less%20than%201%%20of%20scenarios%20might%20genuinely%20need%20“big%20data.”)&setmkt=en-US&count=10",
            "website_link": "https://pigsty.io/blog/pg/pg-eat-db-world/",
            "index": 0,
            "website_title": "Postgres is eating the database world | Pigsty",
            "website_description": " · Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) …",
            "website_text": "Postgres is eating the database world\n\nPostgreSQL isn’t just a simple relational database; it’s a data management framework with the potential to engulf the entire database realm. The trend of “Using Postgres for Everything” is no longer limited to a few elite teams but is becoming a mainstream best practice.\n\n----------------------------------------\n\n\nOLAP’s New Challenger\n\nIn a 2016 database meetup, I argued that a significant gap in the PostgreSQL ecosystem was the lack of a sufficiently good columnar storage engine for OLAP workloads. While PostgreSQL itself offers lots of analysis features, its performance in full-scale analysis on larger datasets doesn’t quite measure up to dedicated real-time data warehouses.\n\nConsider ClickBench, an analytics performance benchmark, where we’ve documented the performance of PostgreSQL, its ecosystem extensions, and derivative databases. The untuned PostgreSQL performs poorly (x1050), but it can reach (x47) with optimization. Additionally, there are three analysis-related extensions: columnar store Hydra (x42), time-series TimescaleDB (x103), and distributed Citus (x262).\n\n\n> Clickbench c6a.4xlarge, 500gb gp2 results in relative time\n\nThis performance can’t be considered bad, especially compared to pure OLTP databases like MySQL and MariaDB (x3065, x19700); however, its third-tier performance is not “good enough,” lagging behind the first-tier OLAP components like Umbra, ClickHouse, Databend, SelectDB (x3~x4) by an order of magnitude. It’s a tough spot - not satisfying enough to use, but too good to discard.\n\nHowever, the arrival of ParadeDB and DuckDB changed the game!\n\nParadeDB’s native PG extension pg_analytics achieves second-tier performance (x10), narrowing the gap to the top tier to just 3–4x. Given the additional benefits, this level of performance discrepancy is often acceptable - ACID, freshness and real-time data without ETL, no additional learning curve, no maintenance of separate services, not to mention its ElasticSearch grade full-text search capabilities.\n\nDuckDB focuses on pure OLAP, pushing analysis performance to the extreme (x3.2) — excluding the academically focused, closed-source database Umbra, DuckDB is arguably the fastest for practical OLAP performance. It’s not a PG extension, but PostgreSQL can fully leverage DuckDB’s analysis performance boost as an embedded file database through projects like DuckDB FDW and pg_quack.\n\nThe emergence of ParadeDB and DuckDB propels PostgreSQL’s analysis capabilities to the top tier of OLAP, filling the last crucial gap in its analytic performance.\n\n----------------------------------------\n\n\nThe Pendulum of Database Realm\n\nThe distinction between OLTP and OLAP didn’t exist at the inception of databases. The separation of OLAP data warehouses from databases emerged in the 1990s due to traditional OLTP databases struggling to support analytics scenarios’ query patterns and performance demands.\n\nFor a long time, best practice in data processing involved using MySQL/PostgreSQL for OLTP workloads and syncing data to specialized OLAP systems like Greenplum, ClickHouse, Doris, Snowflake, etc., through ETL processes.\n\n\n> DDIA ch3, The republic of OLTP & Kingdom of OLAP\n\nLike many “specialized databases,” the strength of dedicated OLAP systems often lies in performance — achieving 1-3 orders of magnitude improvement over native PG or MySQL. The cost, however, is redundant data, excessive data movement, lack of agreement on data values among distributed components, extra labor expense for specialized skills, extra licensing costs, limited query language power, programmability and extensibility, limited tool integration, poor data integrity and availability compared with a complete DMBS.\n\nHowever, as the saying goes, “What goes around comes around”. With hardware improving over thirty years following Moore’s Law, performance has increased exponentially while costs have plummeted. In 2024, a single x86 machine can have hundreds of cores (512 vCPU EPYC 9754x2), several TBs of RAM, a single NVMe SSD can hold up to 64TB, and a single all-flash rack can reach 2PB; object storage like S3 offers virtually unlimited storage.\n\n\nHardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.\n\nAs DuckDB’s manifesto \"Big Data is Dead\" suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.\n\nIf 99% of use cases can now be handled on a single machine with standalone DuckDB or PostgreSQL (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive texts freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)\n\nThe shift in fundamental assumptions is steering the database world from a phase of diversification back to convergence, from a big bang to a mass extinction. In this process, a new era of unified, multi-modeled, super-converged databases will emerge, reuniting OLTP and OLAP. But who will lead this monumental task of reconsolidating the database field?\n\n----------------------------------------\n\n\nPostgreSQL: The Database World Eater\n\nThere are a plethora of niches in the database realm: time-series, geospatial, document, search, graph, vector databases, message queues, and object databases. PostgreSQL makes its presence felt across all these domains.\n\nA case in point is the PostGIS extension, which sets the de facto standard in geospatial databases; the TimescaleDB extension awkwardly positions “generic” time-series databases; and the vector extension, PGVector, turns the dedicated vector database niche into a punchline.\n\nThis isn’t the first time; we’re witnessing it again in the oldest and largest subdomain: OLAP analytics. But PostgreSQL’s ambition doesn’t stop at OLAP; it’s eyeing the entire database world!\n\n\nWhat makes PostgreSQL so capable? Sure, it’s advanced, but so is Oracle; it’s open-source, as is MySQL. PostgreSQL’s edge comes from being both advanced and open-source, allowing it to compete with Oracle/MySQL. But its true uniqueness lies in its extreme extensibility and thriving extension ecosystem.\n\n\n> Reasons users choose PostgreSQL: Open-Source, Reliable, Extensible\n\nPostgreSQL isn’t just a relational database; it’s a data management framework capable of engulfing the entire database galaxy. Besides being open-source and advanced, its core competitiveness stems from extensibility, i.e., its infra’s reusability and extension’s composability.\n\n----------------------------------------\n\n\nThe Magic of Extreme Extensibility\n\nPostgreSQL allows users to develop extensions, leveraging the database’s common infra to deliver features at minimal cost. For instance, the vector database extension pgvector, with just several thousand lines of code, is negligible in complexity compared to PostgreSQL’s millions of lines. Yet, this “insignificant” extension achieves complete vector data types and indexing capabilities, outperforming lots of specialized vector databases.\n\nWhy? Because pgvector’s creators didn’t need to worry about the database’s general additional complexities: ACID, recovery, backup & PITR, high availability, access control, monitoring, deployment, 3rd-party ecosystem tools, client drivers, etc., which require millions of lines of code to solve well. They only focused on the essential complexity of their problem.\n\nFor example, ElasticSearch was developed on the Lucene search library, while the Rust ecosystem has an improved next-gen full-text search library, Tantivy, as a Lucene alternative. ParadeDB only needs to wrap and connect it to PostgreSQL’s interface to offer search services comparable to ElasticSearch. More importantly, it can stand on the shoulders of PostgreSQL, leveraging the entire PG ecosystem’s united strength (e.g., mixed searches with PG Vector) to “unfairly” compete with another dedicated database.\n\n\n> Pigsty has 234 extensions available. And there are 1000+ more in the ecosystem\n\n----------------------------------------\n\nThe extensibility brings another huge advantage: the composability of extensions, allowing different extensions to work together, creating a synergistic effect where 1+1 » 2. For instance, TimescaleDB can be combined with PostGIS for spatio-temporal data support; the BM25 extension for full-text search can be combined with the PGVector extension, providing hybrid search capabilities.\n\nFurthermore, the distributive extension Citus can transparently transform a standalone cluster into a horizontally partitioned distributed database cluster. This capability can be orthogonally combined with other features, making PostGIS a distributed geospatial database, PGVector a distributed vector database, ParadeDB a distributed full-text search database, and so on.\n\n----------------------------------------\n\nWhat’s more powerful is that extensions evolve independently, without the cumbersome need for main branch merges and coordination. This allows for scaling — PG’s extensibility lets numerous teams explore database possibilities in parallel, with all extensions being optional, not affecting the core functionality’s reliability. Those features that are mature and robust have the chance to be stably integrated into the main branch.\n\nPostgreSQL achieves both foundational reliability and agile functionality through the magic of extreme extensibility, making it an outlier in the database world and changing the game rules of the database landscape.\n\n----------------------------------------\n\n\nGame Changer in the DB Arena\n\nThe emergence of PostgreSQL has shifted the paradigms in the database domain: Teams endeavoring to craft a “new database kernel” now face a formidable trial — how to stand out against the open-source, feature-rich Postgres. What’s their unique value proposition?\n\nUntil a revolutionary hardware breakthrough occurs, the advent of practical, new, general-purpose database kernels seems unlikely. No singular database can match the overall prowess of PG, bolstered by all its extensions — not even Oracle, given PG’s ace of being open-source and free.\n\nA niche database product might carve out a space for itself if it can outperform PostgreSQL by an order of magnitude in specific aspects (typically performance). However, it usually doesn’t take long before the PostgreSQL ecosystem spawns open-source extension alternatives. Opting to develop a PG extension rather than a whole new database gives teams a crushing speed advantage in playing catch-up!\n\nFollowing this logic, the PostgreSQL ecosystem is poised to snowball, accruing advantages and inevitably moving towards a monopoly, mirroring the Linux kernel’s status in server OS within a few years. Developer surveys and database trend reports confirm this trajectory.\n\n\n> StackOverflow 2023 Survey: PostgreSQL, the Decathlete\n\n\n> StackOverflow’s Database Trends Over the Past 7 Years\n\nPostgreSQL has long been the favorite database in HackerNews & StackOverflow. Many new open-source projects default to PostgreSQL as their primary, if not only, database choice. And many new-gen companies are going All in PostgreSQL.\n\nAs “Radical Simplicity: Just Use Postgres” says, Simplifying tech stacks, reducing components, accelerating development, lowering risks, and adding more features can be achieved by “Just Use Postgres.” Postgres can replace many backend technologies, including MySQL, Kafka, RabbitMQ, ElasticSearch, Mongo, and Redis, effortlessly serving millions of users. Just Use Postgres is no longer limited to a few elite teams but becoming a mainstream best practice.\n\n----------------------------------------\n\n\nWhat Else Can Be Done?\n\nThe endgame for the database domain seems predictable. But what can we do, and what should we do?\n\nPostgreSQL is already a near-perfect database kernel for the vast majority of scenarios, making the idea of a kernel “bottleneck” absurd. Forks of PostgreSQL and MySQL that tout kernel modifications as selling points are essentially going nowhere.\n\nThis is similar to the situation with the Linux OS kernel today; despite the plethora of Linux distros, everyone opts for the same kernel. Forking the Linux kernel is seen as creating unnecessary difficulties, and the industry frowns upon it.\n\nAccordingly, the main conflict is no longer the database kernel itself but two directions— database extensions and services! The former pertains to internal extensibility, while the latter relates to external composability. Much like the OS ecosystem, the competitive landscape will concentrate on database distributions. In the database domain, only those distributions centered around extensions and services stand a chance for ultimate success.\n\nKernel remains lukewarm, with MariaDB, the fork of MySQL’s parent, nearing delisting, while AWS, profiting from offering services and extensions on top of the free kernel, thrives. Investment has flowed into numerous PG ecosystem extensions and service distributions: Citus, TimescaleDB, Hydra, PostgresML, ParadeDB, FerretDB, StackGres, Aiven, Neon, Supabase, Tembo, PostgresAI, and our own PG distro — — Pigsty.\n\n----------------------------------------\n\nA dilemma within the PostgreSQL ecosystem is the independent evolution of many extensions and tools, lacking a unifier to synergize them. For instance, Hydra releases its own package and Docker image, and so does PostgresML, each distributing PostgreSQL images with their own extensions and only their own. These images and packages are far from comprehensive database services like AWS RDS.\n\nEven service providers and ecosystem integrators like AWS fall short in front of numerous extensions, unable to include many due to various reasons (AGPLv3 license, security challenges with multi-tenancy), thus failing to leverage the synergistic amplification potential of PostgreSQL ecosystem extensions.\n\n> Extesion Category   Pigsty RDS & PGDG     AWS RDS PG             Aliyun RDS PG\n> Add Extension       Free to Install       Not Allowed            Not Allowed\n> Geo Spatial         PostGIS 3.4.2         PostGIS 3.4.1          PostGIS 3.3.4\n> Time Series         TimescaleDB 2.14.2\n> Distributive        Citus 12.1\n> AI / ML             PostgresML 2.8.1\n> Columnar            Hydra 1.1.1\n> Vector              PGVector 0.6          PGVector 0.6           pase 0.0.1\n> Sparse Vector       PG Sparse 0.5.6\n> Full-Text Search    pg_bm25 0.5.6\n> Graph               Apache AGE 1.5.0\n> GraphQL             PG GraphQL 1.5.0\n> Message Queue       pgq 3.5.0\n> OLAP                pg_analytics 0.5.6\n> DuckDB              duckdb_fdw 1.1\n> CDC                 wal2json 2.5.3        wal2json 2.5\n> Bloat Control       pg_repack 1.5.0       pg_repack 1.5.0        pg_repack 1.4.8\n> Point Cloud         PG PointCloud 1.2.5   Ganos PointCloud 6.1\n> \n> Many important extensions are not available on Cloud RDS (PG 16, 2024-02-29)\n\nExtensions are the soul of PostgreSQL. A Postgres without the freedom to use extensions is like cooking without salt, a giant constrained.\n\nAddressing this issue is one of our primary goals.\n\n----------------------------------------\n\n\nOur Resolution: Pigsty\n\nDespite earlier exposure to MySQL Oracle, and MSSQL, when I first used PostgreSQL in 2015, I was convinced of its future dominance in the database realm. Nearly a decade later, I’ve transitioned from a user and administrator to a contributor and developer, witnessing PG’s march toward that goal.\n\nInteractions with diverse users revealed that the database field’s shortcoming isn’t the kernel anymore — PostgreSQL is already sufficient. The real issue is leveraging the kernel’s capabilities, which is the reason behind RDS’s booming success.\n\nHowever, I believe this capability should be as accessible as free software, like the PostgreSQL kernel itself — available to every user, not just renting from cyber feudal lords.\n\nThus, I created Pigsty, a battery-included, local-first PostgreSQL distribution as an open-source RDS Alternative, which aims to harness the collective power of PostgreSQL ecosystem extensions and democratize access to production-grade database services.\n\n\n> Pigsty stands for PostgreSQL in Great STYle, representing the zenith of PostgreSQL.\n\nWe’ve defined six core propositions addressing the central issues in PostgreSQL database services:\n\nExtensible Postgres, Reliable Infras, Observable Graphics, Available Services, Maintainable Toolbox, and Composable Modules.\n\nThe initials of these value propositions offer another acronym for Pigsty:\n\n> Postgres, Infras, Graphics, Service, Toolbox, Yours.\n> \n> Your graphical Postgres infrastructure service toolbox.\n\nExtensible PostgreSQL is the linchpin of this distribution. In the recently launched Pigsty v2.6, we integrated DuckDB FDW and ParadeDB extensions, massively boosting PostgreSQL’s analytical capabilities and ensuring every user can easily harness this power.\n\nOur aim is to integrate the strengths within the PostgreSQL ecosystem, creating a synergistic force akin to the Ubuntu of the database world. I believe the kernel debate is settled, and the real competitive frontier lies here.\n\n * PostGIS: Provides geospatial data types and indexes, the de facto standard for GIS (& pgPointCloud, pgRouting).\n * TimescaleDB: Adds time-series, continuous aggregates, distributed, columnar storage, and automatic compression capabilities.\n * PGVector: Support AI vectors/embeddings and ivfflat, hnsw vector indexes (& pg_sparse for sparse vectors).\n * Citus: Transforms classic master-slave PG clusters into horizontally partitioned distributed database clusters.\n * Hydra: Adds columnar storage and analytics, rivaling ClickHouse’s analytic capabilities.\n * ParadeDB: Elevates full-text search and mixed retrieval to ElasticSearch levels (& zhparser for Chinese tokenization).\n * Apache AGE: Graph database extension, adding Neo4J-like OpenCypher query support to PostgreSQL.\n * PG GraphQL: Adds native built-in GraphQL query language support to PostgreSQL.\n * DuckDB FDW: Enables direct access to DuckDB’s powerful embedded analytic database files through PostgreSQL (& DuckDB CLI).\n * Supabase: An open-source Firebase alternative based on PostgreSQL, providing a complete app development storage solution.\n * FerretDB: An open-source MongoDB alternative based on PostgreSQL, compatible with MongoDB APIs/drivers.\n * PostgresML: Facilitates classic machine learning algorithms, calling, deploying, and training AI models with SQL.\n\n\nDevelopers, your choices will shape the future of the database world. I hope my work helps you better utilize the world’s most advanced open-source database kernel: PostgreSQL.\n\n> Read in Pigsty’s Blog | GitHub Repo: Pigsty | Official Website",
            "source_score": 0.9789473684210527,
            "website_date": "2024-03-04",
            "searched_at": "2024-05-09T13:08:34.844221"
        },
        {
            "text": "Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.As DuckDB’s manifesto “Big Data is Dead” suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.If 99% of use cases can now be handled on a single machine with standalone PostgreSQL / DuckDB (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive text freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)",
            "searched_by_text": "Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.As DuckDB’s manifesto “Big Data is Dead” suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.If 99% of use cases can now be handled on a single machine with standalone PostgreSQL / DuckDB (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive text freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)",
            "query_url": "https://www.bing.com/search?q=Hardware%20advancements%20have%20solved%20the%20data%20volume%20and%20performance%20issue,%20while%20database%20software%20developments%20(PostgreSQL,%20ParadeDB,%20DuckDB)%20have%20addressed%20access%20method%20challenges.%20This%20puts%20the%20fundamental%20assumptions%20of%20the%20analytics%20sector%20—%20the%20so-called%20“big%20data”%20industry%20—%20under%20scrutiny.As%20DuckDB’s%20manifesto%20“Big%20Data%20is%20Dead”%20suggests,%20the%20era%20of%20big%20data%20is%20over.%20Most%20people%20don’t%20have%20that%20much%20data,%20and%20most%20data%20is%20seldom%20queried.%20The%20frontier%20of%20big%20data%20recedes%20as%20hardware%20and%20software%20evolve,%20rendering%20“big%20data”%20unnecessary%20for%2099%%20of%20scenarios.If%2099%%20of%20use%20cases%20can%20now%20be%20handled%20on%20a%20single%20machine%20with%20standalone%20PostgreSQL%20/%20DuckDB%20(and%20its%20replicas),%20what’s%20the%20point%20of%20using%20dedicated%20analytics%20components?%20If%20every%20smartphone%20can%20send%20and%20receive%20text%20freely,%20what’s%20the%20point%20of%20pagers?%20(With%20the%20caveat%20that%20North%20American%20hospitals%20still%20use%20pagers,%20indicating%20that%20maybe%20less%20than%201%%20of%20scenarios%20might%20genuinely%20need%20“big%20data.”)&setmkt=en-US&count=10",
            "website_link": "https://www2.deloitte.com/us/en/insights/industry/technology/challenges-in-data-management.html",
            "index": 1,
            "website_title": "Challenges in data management | Deloitte Insights",
            "website_description": " · If managed poorly, operational and technical challenges can lead to self-inflicted wounds. In a candid discussion with a group of technology industry leaders …",
            "website_text": "Challenges in data management\n\nNo matter the goal, data can be a double-edged sword for technology companies. If wielded with skill, it can cut through competition. If managed poorly, operational and technical challenges can lead to self-inflicted wounds. In a candid discussion with a group of technology industry leaders with data management expertise and influence, we explored their operational maturity, challenges, thoughts on regulation, use of emerging technology, and organizational leading practices.1\n\nMost leaders told us that they had a high degree of confidence in their ability to handle any data management issue, but admitted that challenges still exist around governance, privacy, security, and architecture. Those that felt they were ahead of their peers credited modern applications and infrastructure, multiyear transformation initiatives, data loss prevention solutions, strong organizational collaboration, and robust policies around governance, privacy, and security. The few that felt they were lagging said that they lacked resources to enforce their governance, privacy, and security policies and had struggles with data quality. One leader admitted, “In the spirit of ‘the cobbler's son wears no shoes,’ we are definitely underfunded in IT, with strained resources that are challenged by being leading-edge in our capabilities.”\n\n\nThree key challenges to achieving leaders’ data management goals\n\nCollecting and protecting ever-growing volumes of data ranked as the top barrier (figure). One leader told us, “Volumes continue to rise across all functions, without much regard for prioritizing what data is required to be maintained.” The data leaders highlighted a need for better technologies to collect data, make sense of it, and make it meaningful. They also said that achieving a holistic view of their entire enterprise’s data landscape and identifying sensitive data were significant challenges. Finally, with increasing amounts of data to handle, managing policy implementation and audit was getting harder.\n\nShifting regulations garnered the next-highest ranking. “Regulations changing all the time is a giant pain on top of an already complex problem,” one leader said. Most were resigned to the fact that inconsistencies will arise when trying to satisfy multiple regulators around the world. One said, “All regulations are a cost of doing business; we adapt to them because we don’t have a choice.” Many pointed out that new data-related regulations could lead to higher costs, increased complexity (e.g., network segmentation because of data sovereignty), and challenges to software development.\n\nAs part of the broader regulatory landscape, cross-border transfers and data localization issues also worried our data leaders. Some were trying to avoid or minimize having to transfer data out of country. A few had to establish additional cloud data centers around the world and develop new preferred partners. Generally, there was a fear of unintentionally “tripping up,” because they didn’t know something regarding the dynamic regulatory landscape.\n\nThe third major barrier, cost and complexity of data privacy, was selected by 18% as number one but by 50% as number two, indicating a high level of concern. Data leaders are dealing with a greater focus on data privacy due to customer requests and requirements, regulations, and internal improvements. This is creating a ripple effect, leading to increased costs for leaders, “The cost of everything is rising but our budgets aren't rising with it.”\n\n\nHow leaders are addressing their data management challenges\n\nThe amount of data collected and generated isn’t going to decrease. This means data leaders should anticipate and prepare for potential challenges. Our panelists told us they were focused on four areas:\n\nImproving internal, ecosystem, and industry-wide collaboration. Collaboration is essential for planning data management strategies and facing challenges head-on. Data leaders suggested that clear roles and responsibilities, aligned priorities, and transparent communication and policies were important. Whether collaboration is more structured or ad-hoc, a variety of functional viewpoints is critical: The CIO, research, security, legal, data management, business leadership, application development, infrastructure, and compliance should work together.\n\nStaying on top of rapid regulatory change. It is important to make strategic investments and choices with potential regulatory developments in mind. Having strong, cross-functional collaboration and well-established workflows can help organizations react quickly to regulatory challenges. Some of our data leaders had dedicated teams or roles to monitor and evaluate the impact of global regulations. Modern infrastructure can help as well, one leader told us: “Most of our critical infrastructure is new, automated, and in public cloud, which gives us an advantage in terms of agility. We can redesign and redeploy much of our infrastructure confidently.”\n\nImproving training to increase employee awareness and compliance. Leaders told us that, across the board, there needs to be better user education and stricter enforcement of rules when it comes to data management and cybersecurity. This includes both methodologies and technologies. Data leaders should share in the responsibility as well, by implementing more automated tools. “The average user is not suited to be responsible for understanding the full scope of data they handle and shouldn't be saddled with this level of accountability,” one leader said. “We have to provide more intelligent systems and processes.”\n\nModernizing infrastructure, deploying automated solutions, and experimenting with emerging tech. Many of the data leaders we spoke with wanted to continue to streamline their systems and create a “lighter footprint.” This means moving to a fully SaaS-native cloud and standardizing their tools across all clouds. They also want to integrate new data sources and improve their ability to process real-time information. Some were beginning to experiment with automated data classification, including deployment of homegrown and commercial artificial intelligence/machine learning tools.\n\nIt can be difficult for tech industry leaders to weather the many challenges of increasing data volume, shifting regulations, higher costs, and more complexity. However, by focusing consistently on collaboration, flexibility, modernization, and automation, they may enhance operations and anticipate regulatory requirements.",
            "source_score": 0.06578947368421052,
            "website_date": "2022-09-15",
            "searched_at": "2024-05-09T13:08:34.867222"
        },
        {
            "text": "Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.As DuckDB’s manifesto “Big Data is Dead” suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.If 99% of use cases can now be handled on a single machine with standalone PostgreSQL / DuckDB (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive text freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)",
            "searched_by_text": "Hardware advancements have solved the data volume and performance issue, while database software developments (PostgreSQL, ParadeDB, DuckDB) have addressed access method challenges. This puts the fundamental assumptions of the analytics sector — the so-called “big data” industry — under scrutiny.As DuckDB’s manifesto “Big Data is Dead” suggests, the era of big data is over. Most people don’t have that much data, and most data is seldom queried. The frontier of big data recedes as hardware and software evolve, rendering “big data” unnecessary for 99% of scenarios.If 99% of use cases can now be handled on a single machine with standalone PostgreSQL / DuckDB (and its replicas), what’s the point of using dedicated analytics components? If every smartphone can send and receive text freely, what’s the point of pagers? (With the caveat that North American hospitals still use pagers, indicating that maybe less than 1% of scenarios might genuinely need “big data.”)",
            "query_url": "https://www.bing.com/search?q=Hardware%20advancements%20have%20solved%20the%20data%20volume%20and%20performance%20issue,%20while%20database%20software%20developments%20(PostgreSQL,%20ParadeDB,%20DuckDB)%20have%20addressed%20access%20method%20challenges.%20This%20puts%20the%20fundamental%20assumptions%20of%20the%20analytics%20sector%20—%20the%20so-called%20“big%20data”%20industry%20—%20under%20scrutiny.As%20DuckDB’s%20manifesto%20“Big%20Data%20is%20Dead”%20suggests,%20the%20era%20of%20big%20data%20is%20over.%20Most%20people%20don’t%20have%20that%20much%20data,%20and%20most%20data%20is%20seldom%20queried.%20The%20frontier%20of%20big%20data%20recedes%20as%20hardware%20and%20software%20evolve,%20rendering%20“big%20data”%20unnecessary%20for%2099%%20of%20scenarios.If%2099%%20of%20use%20cases%20can%20now%20be%20handled%20on%20a%20single%20machine%20with%20standalone%20PostgreSQL%20/%20DuckDB%20(and%20its%20replicas),%20what’s%20the%20point%20of%20using%20dedicated%20analytics%20components?%20If%20every%20smartphone%20can%20send%20and%20receive%20text%20freely,%20what’s%20the%20point%20of%20pagers?%20(With%20the%20caveat%20that%20North%20American%20hospitals%20still%20use%20pagers,%20indicating%20that%20maybe%20less%20than%201%%20of%20scenarios%20might%20genuinely%20need%20“big%20data.”)&setmkt=en-US&count=10",
            "website_link": "https://www.coursehero.com/student-questions/35079371-Overview-Technical-standards-are-important-to-ensure-that/",
            "index": 2,
            "website_title": "[Solved] Overview Technical standards are important to ensure …",
            "website_description": "WEBAnswer to Overview Technical standards are important to ensure that all...",
            "website_text": "[Solved] Overview Technical standards are important to ensure that all...\n\nAsked by DelicateRose\n\nOverview\n\nTechnical standards are important to ensure that all development staff utilize the same technologies for consistency, maintenance, and support. By ensuring that all individuals follow a specific set of standards, development processes can be efficiently completed by many different individuals rather than having the dependency on a few.\n\nIn this assignment you complete SDP Section 6, Technical Standards, which is intended to provide developers a high-level view of technologies to be used.\n\nPreparation\n\n * Review the CapraTek Overview found in the assignment Resources as required.\n * Save a new version of your SDP document using this unit number and use it to complete the assignment.\n\nDirections\n \n\nConsider the CapraTek scenario and address the items below in Section 6 of the CapraTek SDP document:\n\n * Create a technology stack component diagram showing where each technical operation is performed.\n * Describe the following components and justify why each is important for solving the identified technical challenges:\n   * Servers: Describe the various types of servers that are appropriate for integration with the identified applications.\n   * Development Software: Identify four development and support tools needed for software development that both address identified technical challenges and that interoperate with Java and .NET operations. Justify your choices.\n\nCapraTek Software Project Overview\n\nImagine that you are a software architect that has been hired by CapraTek to create an IEEE Standard 1058-based Software Development Plan for all their future in-house development projects. The document below has been sent to you by CapraTek's CIO.\n\nHi,\n\nI am excited to have you on board to help us in defining our future software develop practices! I just want to share relevant information about our company. It is not a complete picture, but it is enough to get you started. Please feel free to make and state any assumptions you might need to make in areas that I have not provided sufficient information. We can talk about those when we refine our plan. For now, I need a polished first draft to share with our stakeholders by the end of next month.\n\nWe are a longtime leader in computer server technology but are now shifting our effort to focus on Alfred!, an integrated wireless smart-home hub that seamlessly connects household electronics, appliances, and devices. While we have traditionally outsourced our software development to third party companies, we have now decided to bring all development in-house with the hope that it will decrease design and development time and improve software quality. Many existing projects have either failed, or gone over time and budget, while much of the current software has become stale, due to poorly maintained code, and needs a proper plan.\n\nOne of our biggest challenges is changing the way we produce our product. We are interested in introducing Agile for software development, but many of our personnel are more familiar with traditional project management. Many have a basic understanding of Agile and a keen interest to adopt it, but they need a blueprint from which to work. We understand that there are several Agile methodologies and are looking for guidance on selecting the one that fits best. We hope you can help!\n\nOur Team of Developers\n\nWe currently have 10 in-house developers that have experience with a mixed set of programming languages. Based on previously outsourced work, we are planning to increase this to 20-25 developers, but are not sure of what non-developer resources will be required by Agile. The organization is willing to hire additional non-development support staff (that is, PMs, QA, testers, administrators, etc.) based on Agile requirements.\n\nProjects\n\nWe have three large software development projects planned for next year that are each similar in scope. We need to extend our Alfred! Web application and build both iOS and Android apps for users to control the hub. Using traditional project management techniques, we were able to build our original Alfred! Web application with 6 software developers and one project manager in 12 months. Our plans are to rewrite the entire application to improve the current hub's performance. There will be few added features in the first iteration. The mobile apps will have similar functionality and will be relegated to phones for the first release, although we will be incorporating responsive design that should increase its reach to other devices.\n\nTechnology\n\nWe currently use the following technology and plan to continue to do so:\n\n * Microsoft Xamarin for cross-platform development. \n * C# applications using Visual Studio.\n * ASP.NET Web applications using Visual Studio. \n * Legacy Java applications using NetBeans.\n * Amazon Web Services (AWS).\n * Red Hat Linux.\n * Microsoft Server 2016. \n * Oracle 12c and SQL Server 2016 databases.\n\nHere are some of the requirements and technical challenges facing projects that we would like addressed:\n\n * Servers: Dedicated servers need to be used for each process including databases and Web applications.\n * Bug Tracking: Most of the issues have been dealt with through e-mails or word of mouth. It is important that issues and bugs are tracked in a single application.\n * Version Control: Existing applications, when updated, do not have a trail of code being updated. There needs to be more accountability for developers and ensuring that all changes are being tracked against bug tickets.\n * Asset creation: Any images, videos, or other multimedia needs to be developed in a common tool that can be edited by any other individual. Common standard file formats should be used for any images and videos that are created.\n\n\nI've submitted the capraTek overview along with supporting details already.\n\n\nAnswer & Explanation\n\nsectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus efficitur laoreet. Nam risus ante, dapibus a molestie consequat, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vitae odio. Donec aliquet. Lorem isectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pelle\n\nsum dolor sit amet, consectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibu\n\ns efficitur laoreet. Nam risus ante, dapibus a molestie consequat, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vitae odio. Donec aliquet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus efficitur laoreet. Nam risus ante, dapibus a molestie consequat, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vitae odio. Donec aliq\n\nuet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus efficitur laoreet. Nam risus ante, dapibus a molestie consequat, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vitae odio. Donec aliquet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus effici\n\ntur laoreet. Nam risus ante, dapibus a molestie consequat, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vitae odio. Donec aliquet.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus efficitur laoreet. Nam risus ante, dapibus a molestie consequat, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vitae od\n\n\nio.\nDonec aliquet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus efficitur laoree\nt.\nNam risus ante, dapibus a molestie consequat, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vitae odio. Donec aliquet. Lorem ipsum dolor sit amet, consectetur ad\nipi\nscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus efficitur laoreet. Nam risus ante, dapibus a molestie consequat, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vit\nae\nodio. Donec aliquet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus efficitur laoreet. Nam risus ante, dapibus a molestie consequat\n\n, ultrices ac magna. Fusce dui lectus, congue vel laoreet ac, dictum vitae odio. Donec aliquet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam lacinia pulvinar tortor nec facilisis. Pellentesque dapibus efficitur laoreet. Nam risus ante, dapibus a molestie consequat, ultrices ac",
            "source_score": 0.05423728813559322,
            "website_date": null,
            "searched_at": "2024-05-09T13:08:34.899223"
        }
    ],
    "searched_at": "2024-05-09T13:08:34.899223"
}